{
  "version": 3,
  "sources": ["../lib/model.js", "../lib/learning_rate_defaults.json", "../lib/learning_rates.json", "../lib/loss_functions.json", "../lib/validate.js", "../lib/main.js", "../lib/index.js"],
  "sourcesContent": ["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable no-restricted-syntax, no-invalid-this */\n\n'use strict';\n\n// MODULES //\n\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar setReadOnlyAccessor = require( '@stdlib/utils-define-nonenumerable-read-only-accessor' );\nvar format = require( '@stdlib/string-format' );\nvar gdot = require( '@stdlib/blas-base-gdot' ).ndarray;\nvar gaxpy = require( '@stdlib/blas-base-gaxpy' ).ndarray;\nvar dcopy = require( '@stdlib/blas-base-dcopy' );\nvar dscal = require( '@stdlib/blas-base-dscal' );\nvar max = require( '@stdlib/math-base-special-max' );\nvar exp = require( '@stdlib/math-base-special-exp' );\nvar pow = require( '@stdlib/math-base-special-pow' );\nvar sigmoid = require( '@stdlib/math-base-special-expit' );\nvar Float64Array = require( '@stdlib/array-float64' );\nvar ndarray = require( '@stdlib/ndarray-ctor' );\nvar shape2strides = require( '@stdlib/ndarray-base-shape2strides' );\nvar numel = require( '@stdlib/ndarray-base-numel' );\nvar vind2bind = require( '@stdlib/ndarray-base-vind2bind' );\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1.0e-7;\nvar MIN_SCALE = 1.0e-11;\nvar LEARNING_RATE_METHODS = {\n\t'basic': '_basicLearningRate',\n\t'constant': '_constantLearningRate',\n\t'invscaling': '_inverseScalingLearningRate',\n\t'pegasos': '_pegasosLearningRate'\n};\nvar LOSS_METHODS = {\n\t'hinge': '_hingeLoss',\n\t'log': '_logLoss',\n\t'modifiedHuber': '_modifiedHuberLoss',\n\t'perceptron': '_perceptronLoss',\n\t'squaredHinge': '_squaredHingeLoss'\n};\n\n\n// MAIN //\n\n/**\n* Model constructor.\n*\n* ## Notes\n*\n* -   The model (weight vector) implementation is inspired by the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @constructor\n* @param {PositiveInteger} N - number of feature weights (excluding bias/intercept term)\n* @param {Options} opts - model options\n* @param {PositiveNumber} opts.lambda - regularization parameter\n* @param {ArrayLikeObject} opts.learningRate - learning rate function and associated parameters\n* @param {string} opts.loss - loss function\n* @param {boolean} opts.intercept - boolean indicating whether to include an intercept\n* @returns {Model} model\n*/\nfunction Model( N, opts ) {\n\tvar len;\n\n\t// Set internal properties:\n\tthis._N = N;\n\tthis._opts = opts;\n\n\tthis._scaleFactor = 1.0;\n\tthis._t = 0; // iteration counter (i.e., number of updates)\n\n\t// Determine the learning rate function:\n\tthis._learningRateMethod = LEARNING_RATE_METHODS[ opts.learningRate[ 0 ] ];\n\n\t// Determine the loss function:\n\tthis._lossMethod = LOSS_METHODS[ opts.loss ];\n\n\t// Determine the number of model coefficients:\n\tlen = N;\n\tif ( opts.intercept ) {\n\t\tlen += 1;\n\t}\n\t// Initialize a model weight vector with all weights set to zero:\n\tthis._weights = new Float64Array( len );\n\n\t// Initialize model coefficients to zero:\n\tthis._coefficients = new ndarray( 'float64', new Float64Array( len ), [ len ], [ 1 ], 0, 'row-major' );\n\n\treturn this;\n}\n\n/**\n* Adds a provided input vector to the model weight vector.\n*\n* @private\n* @name _add\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - input vector\n* @param {number} scale - scale factor\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_add', function add( x, scale ) {\n\tvar s = scale / this._scaleFactor;\n\tvar w = this._weights;\n\n\t// Scale `x` and add to the model weight vector:\n\tgaxpy( x.shape[ 0 ], s, x.data, x.strides[ 0 ], x.offset, w, 1, 0 );\n\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this._opts.intercept ) {\n\t\tw[ this._N ] += s;\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate.\n*\n* ## Notes\n*\n* -   This learning rate function is based on the learning rate function of the same name in the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @name _basicLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_basicLearningRate', function basic() {\n\treturn 10.0 / ( 10.0+this._t );\n});\n\n/**\n* Returns a constant learning rate.\n*\n* @private\n* @name _constantLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_constantLearningRate', function constant() {\n\treturn this._opts.learningRate[ 1 ];\n});\n\n/**\n* Calculates the dot product of the model weight vector and a provided vector `x`.\n*\n* @private\n* @name _dot\n* @memberof Model.prototype\n* @type {Function}\n* @param {NumericArray} buf - ndarray data buffer\n* @param {integer} stride - stride\n* @param {NonNegativeInteger} offset - index offset\n* @returns {number} dot product\n*/\nsetReadOnly( Model.prototype, '_dot', function dot( buf, stride, offset ) {\n\tvar v = gdot( this._N, this._weights, 1, 0, buf, stride, offset );\n\tif ( this._opts.intercept ) {\n\t\tv += this._weights[ this._N ];\n\t}\n\tv *= this._scaleFactor;\n\treturn v;\n});\n\n/**\n* Updates the model weight vector using the hinge loss function.\n*\n* ## Notes\n*\n* -   The hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _hingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_hingeLoss', function hingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) < 1.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate according to an inverse scaling formula.\n*\n* ## Notes\n*\n* -   The inverse scaling formula is defined as\n*\n*     ```tex\n*     \\eta = \\frac{\\eta_0}{t^{k}}\n*     ```\n*\n*     where \\\\(\\eta_0\\\\) is an initial learning rate, \\\\(t\\\\) is the current iteration, and \\\\(k\\\\) is an exponent controlling how quickly the learning rate decreases.\n*\n* @private\n* @name _inverseScalingLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_inverseScalingLearningRate', function invscaling() {\n\tvar params = this._opts.learningRate;\n\treturn params[ 1 ] / pow( this._t, params[ 2 ] );\n});\n\n/**\n* Updates the model weight vector using the log loss function.\n*\n* ## Notes\n*\n* -   The log loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\ln( 1 + \\exp( -y\\,f(x) ) )\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _logLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_logLoss', function logLoss( x, y ) {\n\tvar loss;\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tloss = y / ( 1.0 + exp( y*d ) );\n\tthis._add( x, eta*loss );\n\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the modified Huber loss function.\n*\n* ## Notes\n*\n* -   The modified Huber loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\begin{cases}\n*       \\max(0, 1 - y\\,f(x))^2 & \\textrm{for}\\,\\,y\\,f(x) \\geq -1\\\\\n*       -4y\\,f(x) & \\textrm{otherwise}\n*     \\end{cases}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* ## References\n*\n* -   Zhang, Tong. 2004. \"Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms.\" In _Proceedings of the Twenty-First International Conference on Machine Learning_, 116. New York, NY, USA: Association for Computing Machinery. doi:[10.1145/1015330.1015332][@zhang:2004a].\n*\n* [@zhang:2004a]: https://doi.org/10.1145/1015330.1015332\n*\n* @private\n* @name _modifiedHuberLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_modifiedHuberLoss', function modifiedHuber( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < -1.0 ) {\n\t\tthis._add( x, 4.0*eta*y );\n\t} else {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate using Pegasos.\n*\n* ## References\n*\n* -   Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. \"Pegasos: primal estimated sub-gradient solver for SVM.\" _Mathematical Programming_ 127 (1): 3\u201330. doi:[10.1007/s10107-010-0420-4][@shalevshwartz:2011a].\n*\n* [@shalevshwartz:2011a]: https://doi.org/10.1007/s10107-010-0420-4\n*\n* @private\n* @name _pegasos\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_pegasosLearningRate', function pegasos() {\n\treturn 1.0 / ( this._opts.lambda*this._t );\n});\n\n/**\n* Updates the model weight vector using the perceptron loss function.\n*\n* ## Notes\n*\n* -   The perceptron loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max(0, -y\\,f(x))\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* -   The perceptron loss function is equivalent to the hinge loss function without a margin.\n*\n* -   The perceptron loss function does not update the model weight vector when the response is correctly classified.\n*\n* ## References\n*\n* -   Rosenblatt, Frank. 1957. \"The Perceptron\u2013a perceiving and recognizing automaton.\" 85-460-1. Buffalo, NY, USA: Cornell Aeronautical Laboratory.\n*\n* @private\n* @name _perceptronLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_perceptronLoss', function perceptron( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) <= 0.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Performs L2 regularization of the model weights.\n*\n* @private\n* @name _regularize\n* @memberof Model.prototype\n* @type {Function}\n* @param {PositiveNumber} eta - learning rate\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_regularize', function regularize( eta ) {\n\tvar lambda = this._opts.lambda;\n\tif ( lambda <= 0.0 ) {\n\t\treturn this;\n\t}\n\tthis._scale( max( 1.0-( eta*lambda ), MIN_SCALING_FACTOR ) );\n\treturn this;\n});\n\n/**\n* Scale the model weight vector by a provided scaling factor.\n*\n* @private\n* @name _scale\n* @memberof Model.prototype\n* @type {Function}\n* @param {number} factor - scaling factor\n* @throws {RangeError} scaling factor must be a positive number\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_scale', function scale( factor ) {\n\tvar s;\n\tif ( factor <= 0.0 ) {\n\t\tthrow new RangeError( format( 'invalid argument. Attempting to scale a weight vector by a nonpositive value. This is likely due to too large a value of eta * lambda. Value: `%f`.', factor ) );\n\t}\n\t// Check whether we need to scale the weight vector to unity in order to avoid numerical issues...\n\ts = this._scaleFactor;\n\tif ( s < MIN_SCALE ) {\n\t\t// Note: we only scale/shrink the feature weights, not the intercept...\n\t\tdscal( this._N, s, this._weights, 1 );\n\t\tthis._scaleFactor = 1.0;\n\t}\n\tthis._scaleFactor *= factor;\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the squared hinge loss function.\n*\n* ## Notes\n*\n* -   The squared hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}^2\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _squaredHingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_squaredHingeLoss', function squaredHingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < 1.0 ) {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Returns the model coefficients.\n*\n* @private\n* @name coefficients\n* @memberof Model.prototype\n* @type {Function}\n* @returns {ndarray} model coefficients\n*/\nsetReadOnlyAccessor( Model.prototype, 'coefficients', function coefficients() {\n\tvar c = this._coefficients.data;\n\tvar w = this._weights;\n\tdcopy( w.length, w, 1, c, 1 );\n\tdscal( this._N, this._scaleFactor, c, 1 );\n\treturn this._coefficients;\n});\n\n/**\n* Returns the number of model features.\n*\n* @private\n* @name nfeatures\n* @memberof Model.prototype\n* @type {PositiveInteger}\n*/\nsetReadOnlyAccessor( Model.prototype, 'nfeatures', function nfeatures() {\n\treturn this._N;\n});\n\n/**\n* Predicts the response value for one or more observation vectors `X`.\n*\n* @private\n* @name predict\n* @memberof Model.prototype\n* @type {Function}\n* @param {ndarray} X - feature vector\n* @param {string} type - prediction type\n* @returns {ndarray} ndarray containing response values\n*/\nsetReadOnly( Model.prototype, 'predict', function predict( X, type ) {\n\tvar ndims;\n\tvar xbuf;\n\tvar ybuf;\n\tvar xsh;\n\tvar ysh;\n\tvar ord;\n\tvar ptr;\n\tvar sxn;\n\tvar sx;\n\tvar sy;\n\tvar ox;\n\tvar M;\n\tvar N;\n\tvar Y;\n\tvar v;\n\tvar i;\n\n\t// Cache input array properties in case of lazy evaluation:\n\txbuf = X.data;\n\txsh = X.shape;\n\tsx = X.strides;\n\tox = X.offset;\n\tord = X.order;\n\n\tndims = xsh.length - 1;\n\n\t// The output array shape is the same as the input array shape without the last dimension (i.e., the number of dimensions is reduced by one)...\n\tysh = [];\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tysh.push( xsh[ i ] );\n\t}\n\t// Create an output array...\n\tif ( ndims === 0 ) {\n\t\tM = 1;\n\t\tybuf = new Float64Array( 1 );\n\t\tsy = [ 0 ];\n\t} else {\n\t\tM = numel( ysh );\n\t\tybuf = new Float64Array( M );\n\t\tsy = shape2strides( ysh, ord );\n\t}\n\tY = new ndarray( 'int8', ybuf, ysh, sy, 0, ord );\n\n\t// Loop over all observation vectors...\n\tN = this._N; // number of features (i.e., size of last `X` dimension)\n\tsxn = sx[ ndims ]; // stride of the last `X` dimension\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Compute the index offset into the underlying data buffer pointing to the start of the current observation vector:\n\t\tptr = vind2bind( xsh, sx, ox, ord, i*N, 'throw' );\n\n\t\t// Compute the dot product of the current observation vector with the model weight vector:\n\t\tv = this._dot( xbuf, sxn, ptr );\n\n\t\t// Determine the output value:\n\t\tif ( type === 'label' ) {\n\t\t\tv = ( v > 0 ) ? 1 : -1;\n\t\t} else if ( type === 'probability' ) {\n\t\t\tv = sigmoid( v );\n\t\t} // else type === 'linear' (i.e., linear predictor)\n\n\t\t// Set the element in the output array:\n\t\tif ( ndims === 0 ) {\n\t\t\tY.iset( v );\n\t\t} else {\n\t\t\tY.iset( i, v );\n\t\t}\n\t}\n\treturn Y;\n});\n\n/**\n* Updates a model given a provided observation vector and response value.\n*\n* @private\n* @name update\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, 'update', function update( x, y ) {\n\tthis._t += 1;\n\treturn this[ this._lossMethod ]( x, y );\n});\n\n\n// EXPORTS //\n\nmodule.exports = Model;\n", "{\n\t\"basic\": [ \"basic\" ],\n\t\"constant\": [ \"constant\", 0.02 ],\n\t\"invscaling\": [ \"invscaling\", 0.02, 0.5 ],\n\t\"pegasos\": [ \"pegasos\" ]\n}\n", "[\n\t\"basic\",\n\t\"constant\",\n\t\"invscaling\",\n\t\"pegasos\"\n]\n", "[\n\t\"hinge\",\n\t\"log\",\n\t\"modifiedHuber\",\n\t\"perceptron\",\n\t\"squaredHinge\"\n]\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar isNonNegativeNumber = require( '@stdlib/assert-is-nonnegative-number' ).isPrimitive;\nvar isPositiveNumber = require( '@stdlib/assert-is-positive-number' ).isPrimitive;\nvar isNumber = require( '@stdlib/assert-is-number' ).isPrimitive;\nvar isBoolean = require( '@stdlib/assert-is-boolean' ).isPrimitive;\nvar isArrayLikeObject = require( '@stdlib/assert-is-array-like-object' );\nvar isObject = require( '@stdlib/assert-is-plain-object' );\nvar hasOwnProp = require( '@stdlib/assert-has-own-property' );\nvar contains = require( '@stdlib/assert-contains' );\nvar format = require( '@stdlib/string-format' );\nvar LEARNING_RATES = require( './learning_rates.json' );\nvar LOSS_FUNCTIONS = require( './loss_functions.json' );\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate] - learning rate function\n* @param {string} [options.loss] - loss function\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tvar name;\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( 'invalid argument. Options argument must be an object. Value: `%s`.', options ) );\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a nonnegative number. Option: `%s`.', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\tif ( !isArrayLikeObject( options.learningRate ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be an array-like object. Option: `%s`.', 'learningRate', options.learningRate ) );\n\t\t}\n\t\tname = options.learningRate[ 0 ];\n\t\topts.learningRate[ 0 ] = name;\n\t\tif ( !contains( LEARNING_RATES, name ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. First `%s` option must be one of the following: \"%s\". Option: `%s`.', 'learningRate', LEARNING_RATES.join( '\", \"' ), name ) );\n\t\t}\n\t\tif ( options.learningRate.length > 1 ) {\n\t\t\tif ( name === 'constant' || name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 1 ] = options.learningRate[ 1 ];\n\t\t\t\tif ( !isPositiveNumber( opts.learningRate[ 1 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Second `%s` option must be a positive number. Option: `%s`.', 'learningRate', opts.learningRate[ 1 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ( options.learningRate.length > 2 ) {\n\t\t\tif ( name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 2 ] = options.learningRate[ 2 ];\n\t\t\t\tif ( !isNumber( opts.learningRate[ 2 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Third `%s` option must be a number. Option: `%s`.', 'learningRate', opts.learningRate[ 2 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !contains( LOSS_FUNCTIONS, opts.loss ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'loss', LOSS_FUNCTIONS.join( '\", \"' ), opts.loss ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nmodule.exports = validate;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar isPositiveInteger = require( '@stdlib/assert-is-positive-integer' ).isPrimitive;\nvar isVectorLike = require( '@stdlib/assert-is-vector-like' );\nvar isndarrayLike = require( '@stdlib/assert-is-ndarray-like' );\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar format = require( '@stdlib/string-format' );\nvar Model = require( './model.js' );\nvar LEARNING_RATE_DEFAULTS = require( './learning_rate_defaults.json' );\nvar validate = require( './validate.js' );\n\n\n// MAIN //\n\n/**\n* Returns an accumulator function which incrementally performs binary classification using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* -   The sub-gradient of the loss function is estimated for each datum and the classification model is updated incrementally, with a decreasing learning rate and regularization of model feature weights using L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3\u201330. doi:10.1007/s10107-010-0420-4\n*\n* @param {PositiveInteger} N - number of features\n* @param {Options} [options] - options object\n* @param {PositiveNumber} [options.lambda=1.0e-3] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate=['basic']] - learning rate function and associated parameters (one of `basic`, `constant`, or `pegasos`)\n* @param {string} [options.loss='log'] - loss function (one of `hinge`, `log`, `modifiedHuber`, `perceptron`, or `squaredHinge`)\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Function} accumulator\n*\n* @example\n* var Float64Array = require( '@stdlib/array-float64' );\n* var array = require( '@stdlib/ndarray-array' );\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\nfunction incrBinaryClassification( N, options ) {\n\tvar model;\n\tvar opts;\n\tvar err;\n\n\tif ( !isPositiveInteger( N ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. First argument must be a positive integer. Value: `%s`.', N ) );\n\t}\n\topts = {\n\t\t'intercept': true,\n\t\t'lambda': 1.0e-4,\n\t\t'learningRate': LEARNING_RATE_DEFAULTS[ 'basic' ].slice(),\n\t\t'loss': 'log'\n\t};\n\tif ( arguments.length > 1 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\tmodel = new Model( N, opts );\n\n\t// Attach methods to the accumulator:\n\tsetReadOnly( accumulator, 'predict', predict );\n\n\treturn accumulator;\n\n\t/**\n\t* If provided a feature vector and response value, the accumulator function updates a binary classification model; otherwise, the accumulator function returns the current binary classification model coefficients.\n\t*\n\t* @private\n\t* @param {VectorLike} x - feature vector\n\t* @param {integer} y - response value\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray whose length matches the number of model features\n\t* @throws {TypeError} second argument must be either `+1` or `-1`\n\t* @returns {ndarray} one-dimensional ndarray containing model coefficients\n\t*\n\t* @example\n\t* var Float64Array = require( '@stdlib/array-float64' );\n\t* var array = require( '@stdlib/ndarray-array' );\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Update the model:\n\t* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n\t* var coefs = accumulator( x, 1 );\n\t* // returns <ndarray>\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tif ( arguments.length === 0 ) {\n\t\t\treturn model.coefficients;\n\t\t}\n\t\tif ( !isVectorLike( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( y !== -1 && y !== 1 ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be either +1 or -1. Value: `%s`.', y ) );\n\t\t}\n\t\tif ( x.shape[ 0 ] !== model.nfeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray of length %u. Actual length: `%u`.', model.nfeatures, x.shape[ 0 ] ) );\n\t\t}\n\t\tmodel.update( x, y );\n\t\treturn model.coefficients;\n\t}\n\n\t/**\n\t* Predicts the response value for one or more observation vectors `X`.\n\t*\n\t* @private\n\t* @param {ndarrayLike} X - ndarray (of size `(...,N)`) containing observation vectors\n\t* @param {string} [type=\"label\"] - prediction type (either `label`, `probability`, or `linear`)\n\t* @throws {TypeError} first argument must be an ndarray\n\t* @throws {TypeError} first argument must be an ndarray whose last dimension matches the number of model features\n\t* @throws {TypeError} second argument must be a recognized/supported prediction \"type\"\n\t* @throws {Error} second argument must be compatible with the model loss function\n\t* @returns {ndarray} ndarray (of size `(...)`) containing response values\n\t*\n\t* @example\n\t* var Float64Array = require( '@stdlib/array-float64' );\n\t* var array = require( '@stdlib/ndarray-array' );\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Create a new observation vector:\n\t* var x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n\t*\n\t* // Predict the response value:\n\t* var yhat = accumulator.predict( x );\n\t* // returns <ndarray>\n\t*/\n\tfunction predict( X, type ) {\n\t\tvar sh;\n\t\tvar t;\n\t\tif ( !isndarrayLike( X ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray. Value: `%s`.', X ) );\n\t\t}\n\t\tsh = X.shape;\n\t\tif ( sh[ sh.length-1 ] !== N ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray whose last dimension is of size %u. Actual size: `%u`.', N, sh[ sh.length-1 ] ) );\n\t\t}\n\t\tt = 'label';\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( type === 'probability' ) {\n\t\t\t\tif ( opts.loss !== 'log' && opts.loss !== 'modifiedHuber' ) {\n\t\t\t\t\tthrow new Error( format( 'invalid argument. Second argument is incompatible with model loss function. Probability predictions are only supported when the loss function is one of the following: \"%s\". Model loss function: `%s`.', [ 'log', 'modifiedHuber' ].join( '\", \"' ), opts.loss ) );\n\t\t\t\t}\n\t\t\t} else if ( type !== 'label' && type !== 'linear' ) {\n\t\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be a string value equal to either \"label\", \"probability\", or \"linear\". Value: `%s`.', type ) );\n\t\t\t}\n\t\t\tt = type;\n\t\t}\n\t\treturn model.predict( X, t );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = incrBinaryClassification;\n", "/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Incrementally perform binary classification using stochastic gradient descent (SGD).\n*\n* @module @stdlib/ml-incr-binary-classification\n*\n* @example\n* var Float64Array = require( '@stdlib/array-float64' );\n* var array = require( '@stdlib/ndarray-array' );\n* var incrBinaryClassification = require( '@stdlib/ml-incr-binary-classification' );\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\n\n// MODULES //\n\nvar main = require( './main.js' );\n\n\n// EXPORTS //\n\nmodule.exports = main;\n"],
  "mappings": "uGAAA,IAAAA,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAwBA,IAAIC,EAAc,QAAS,uDAAwD,EAC/EC,EAAsB,QAAS,uDAAwD,EACvFC,EAAS,QAAS,uBAAwB,EAC1CC,EAAO,QAAS,wBAAyB,EAAE,QAC3CC,EAAQ,QAAS,yBAA0B,EAAE,QAC7CC,EAAQ,QAAS,yBAA0B,EAC3CC,EAAQ,QAAS,yBAA0B,EAC3CC,EAAM,QAAS,+BAAgC,EAC/CC,EAAM,QAAS,+BAAgC,EAC/CC,EAAM,QAAS,+BAAgC,EAC/CC,EAAU,QAAS,iCAAkC,EACrDC,EAAe,QAAS,uBAAwB,EAChDC,EAAU,QAAS,sBAAuB,EAC1CC,EAAgB,QAAS,oCAAqC,EAC9DC,EAAQ,QAAS,4BAA6B,EAC9CC,EAAY,QAAS,gCAAiC,EAKtDC,EAAqB,KACrBC,EAAY,MACZC,GAAwB,CAC3B,MAAS,qBACT,SAAY,wBACZ,WAAc,8BACd,QAAW,sBACZ,EACIC,GAAe,CAClB,MAAS,aACT,IAAO,WACP,cAAiB,qBACjB,WAAc,kBACd,aAAgB,mBACjB,EAwBA,SAASC,EAAOC,EAAGC,EAAO,CACzB,IAAIC,EAGJ,YAAK,GAAKF,EACV,KAAK,MAAQC,EAEb,KAAK,aAAe,EACpB,KAAK,GAAK,EAGV,KAAK,oBAAsBJ,GAAuBI,EAAK,aAAc,CAAE,CAAE,EAGzE,KAAK,YAAcH,GAAcG,EAAK,IAAK,EAG3CC,EAAMF,EACDC,EAAK,YACTC,GAAO,GAGR,KAAK,SAAW,IAAIZ,EAAcY,CAAI,EAGtC,KAAK,cAAgB,IAAIX,EAAS,UAAW,IAAID,EAAcY,CAAI,EAAG,CAAEA,CAAI,EAAG,CAAE,CAAE,EAAG,EAAG,WAAY,EAE9F,IACR,CAaAvB,EAAaoB,EAAM,UAAW,OAAQ,SAAcI,EAAGC,EAAQ,CAC9D,IAAIC,EAAID,EAAQ,KAAK,aACjBE,EAAI,KAAK,SAGb,OAAAvB,EAAOoB,EAAE,MAAO,CAAE,EAAGE,EAAGF,EAAE,KAAMA,EAAE,QAAS,CAAE,EAAGA,EAAE,OAAQG,EAAG,EAAG,CAAE,EAG7D,KAAK,MAAM,YACfA,EAAG,KAAK,EAAG,GAAKD,GAEV,IACR,CAAC,EAiBD1B,EAAaoB,EAAM,UAAW,qBAAsB,UAAiB,CACpE,MAAO,KAAS,GAAK,KAAK,GAC3B,CAAC,EAWDpB,EAAaoB,EAAM,UAAW,wBAAyB,UAAoB,CAC1E,OAAO,KAAK,MAAM,aAAc,CAAE,CACnC,CAAC,EAcDpB,EAAaoB,EAAM,UAAW,OAAQ,SAAcQ,EAAKC,EAAQC,EAAS,CACzE,IAAIC,EAAI5B,EAAM,KAAK,GAAI,KAAK,SAAU,EAAG,EAAGyB,EAAKC,EAAQC,CAAO,EAChE,OAAK,KAAK,MAAM,YACfC,GAAK,KAAK,SAAU,KAAK,EAAG,GAE7BA,GAAK,KAAK,aACHA,CACR,CAAC,EA6BD/B,EAAaoB,EAAM,UAAW,aAAc,SAAoBI,EAAGQ,EAAI,CACtE,IAAIC,EACAC,EAEJ,OAAAD,EAAM,KAAM,KAAK,mBAAoB,EAAE,EACvC,KAAK,YAAaA,CAAI,EAEtBC,EAAI,KAAK,KAAMV,EAAE,KAAMA,EAAE,QAAS,CAAE,EAAGA,EAAE,MAAO,EACzCQ,EAAEE,EAAM,GACd,KAAK,KAAMV,EAAGQ,EAAEC,CAAI,EAEd,IACR,CAAC,EAqBDjC,EAAaoB,EAAM,UAAW,8BAA+B,UAAsB,CAClF,IAAIe,EAAS,KAAK,MAAM,aACxB,OAAOA,EAAQ,CAAE,EAAI1B,EAAK,KAAK,GAAI0B,EAAQ,CAAE,CAAE,CAChD,CAAC,EA6BDnC,EAAaoB,EAAM,UAAW,WAAY,SAAkBI,EAAGQ,EAAI,CAClE,IAAII,EACAH,EACAC,EAEJ,OAAAD,EAAM,KAAM,KAAK,mBAAoB,EAAE,EACvC,KAAK,YAAaA,CAAI,EAEtBC,EAAI,KAAK,KAAMV,EAAE,KAAMA,EAAE,QAAS,CAAE,EAAGA,EAAE,MAAO,EAChDY,EAAOJ,GAAM,EAAMxB,EAAKwB,EAAEE,CAAE,GAC5B,KAAK,KAAMV,EAAGS,EAAIG,CAAK,EAEhB,IACR,CAAC,EAsCDpC,EAAaoB,EAAM,UAAW,qBAAsB,SAAwBI,EAAGQ,EAAI,CAClF,IAAIC,EACAC,EAEJ,OAAAD,EAAM,KAAM,KAAK,mBAAoB,EAAE,EACvC,KAAK,YAAaA,CAAI,EAEtBC,EAAIF,EAAI,KAAK,KAAMR,EAAE,KAAMA,EAAE,QAAS,CAAE,EAAGA,EAAE,MAAO,EAC/CU,EAAI,GACR,KAAK,KAAMV,EAAG,EAAIS,EAAID,CAAE,EAExB,KAAK,KAAMR,EAAGS,GAAMD,EAAGE,EAAEF,EAAK,EAExB,IACR,CAAC,EAiBDhC,EAAaoB,EAAM,UAAW,uBAAwB,UAAmB,CACxE,MAAO,IAAQ,KAAK,MAAM,OAAO,KAAK,GACvC,CAAC,EAqCDpB,EAAaoB,EAAM,UAAW,kBAAmB,SAAqBI,EAAGQ,EAAI,CAC5E,IAAIC,EACAC,EAEJ,OAAAD,EAAM,KAAM,KAAK,mBAAoB,EAAE,EACvC,KAAK,YAAaA,CAAI,EAEtBC,EAAI,KAAK,KAAMV,EAAE,KAAMA,EAAE,QAAS,CAAE,EAAGA,EAAE,MAAO,EACzCQ,EAAEE,GAAO,GACf,KAAK,KAAMV,EAAGQ,EAAEC,CAAI,EAEd,IACR,CAAC,EAYDjC,EAAaoB,EAAM,UAAW,cAAe,SAAqBa,EAAM,CACvE,IAAII,EAAS,KAAK,MAAM,OACxB,OAAKA,GAAU,EACP,MAER,KAAK,OAAQ9B,EAAK,EAAM0B,EAAII,EAAUrB,CAAmB,CAAE,EACpD,KACR,CAAC,EAaDhB,EAAaoB,EAAM,UAAW,SAAU,SAAgBkB,EAAS,CAChE,IAAIZ,EACJ,GAAKY,GAAU,EACd,MAAM,IAAI,WAAYpC,EAAQ,sJAAuJoC,CAAO,CAAE,EAG/L,OAAAZ,EAAI,KAAK,aACJA,EAAIT,IAERX,EAAO,KAAK,GAAIoB,EAAG,KAAK,SAAU,CAAE,EACpC,KAAK,aAAe,GAErB,KAAK,cAAgBY,EACd,IACR,CAAC,EA6BDtC,EAAaoB,EAAM,UAAW,oBAAqB,SAA2BI,EAAGQ,EAAI,CACpF,IAAIC,EACAC,EAEJ,OAAAD,EAAM,KAAM,KAAK,mBAAoB,EAAE,EACvC,KAAK,YAAaA,CAAI,EAEtBC,EAAIF,EAAI,KAAK,KAAMR,EAAE,KAAMA,EAAE,QAAS,CAAE,EAAGA,EAAE,MAAO,EAC/CU,EAAI,GACR,KAAK,KAAMV,EAAGS,GAAMD,EAAGE,EAAEF,EAAK,EAExB,IACR,CAAC,EAWD/B,EAAqBmB,EAAM,UAAW,eAAgB,UAAwB,CAC7E,IAAImB,EAAI,KAAK,cAAc,KACvBZ,EAAI,KAAK,SACb,OAAAtB,EAAOsB,EAAE,OAAQA,EAAG,EAAGY,EAAG,CAAE,EAC5BjC,EAAO,KAAK,GAAI,KAAK,aAAciC,EAAG,CAAE,EACjC,KAAK,aACb,CAAC,EAUDtC,EAAqBmB,EAAM,UAAW,YAAa,UAAqB,CACvE,OAAO,KAAK,EACb,CAAC,EAaDpB,EAAaoB,EAAM,UAAW,UAAW,SAAkBoB,EAAGC,EAAO,CACpE,IAAIC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAhC,EACAiC,EACA,EACAC,EAaJ,IAVAZ,EAAOH,EAAE,KACTK,EAAML,EAAE,MACRU,EAAKV,EAAE,QACPY,EAAKZ,EAAE,OACPO,EAAMP,EAAE,MAERE,EAAQG,EAAI,OAAS,EAGrBC,EAAM,CAAC,EACDS,EAAI,EAAGA,EAAIb,EAAOa,IACvBT,EAAI,KAAMD,EAAKU,CAAE,CAAE,EAiBpB,IAdKb,IAAU,GACdW,EAAI,EACJT,EAAO,IAAIjC,EAAc,CAAE,EAC3BwC,EAAK,CAAE,CAAE,IAETE,EAAIvC,EAAOgC,CAAI,EACfF,EAAO,IAAIjC,EAAc0C,CAAE,EAC3BF,EAAKtC,EAAeiC,EAAKC,CAAI,GAE9BO,EAAI,IAAI1C,EAAS,OAAQgC,EAAME,EAAKK,EAAI,EAAGJ,CAAI,EAG/C1B,EAAI,KAAK,GACT4B,EAAMC,EAAIR,CAAM,EACVa,EAAI,EAAGA,EAAIF,EAAGE,IAEnBP,EAAMjC,EAAW8B,EAAKK,EAAIE,EAAIL,EAAKQ,EAAElC,EAAG,OAAQ,EAGhD,EAAI,KAAK,KAAMsB,EAAMM,EAAKD,CAAI,EAGzBP,IAAS,QACb,EAAM,EAAI,EAAM,EAAI,GACTA,IAAS,gBACpB,EAAI/B,EAAS,CAAE,GAIXgC,IAAU,EACdY,EAAE,KAAM,CAAE,EAEVA,EAAE,KAAMC,EAAG,CAAE,EAGf,OAAOD,CACR,CAAC,EAaDtD,EAAaoB,EAAM,UAAW,SAAU,SAAiBI,EAAGQ,EAAI,CAC/D,YAAK,IAAM,EACJ,KAAM,KAAK,WAAY,EAAGR,EAAGQ,CAAE,CACvC,CAAC,EAKDjC,EAAO,QAAUqB,IC1nBjB,IAAAoC,EAAAC,EAAA,SAAAC,GAAAC,GAAA,CAAAA,GAAA,SACC,MAAS,CAAE,OAAQ,EACnB,SAAY,CAAE,WAAY,GAAK,EAC/B,WAAc,CAAE,aAAc,IAAM,EAAI,EACxC,QAAW,CAAE,SAAU,CACxB,ICLA,IAAAC,EAAAC,EAAA,SAAAC,GAAAC,GAAA,CAAAA,GAAA,SACC,QACA,WACA,aACA,SACD,ICLA,IAAAC,EAAAC,EAAA,SAAAC,GAAAC,GAAA,CAAAA,GAAA,SACC,QACA,MACA,gBACA,aACA,cACD,ICNA,IAAAC,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAsB,QAAS,sCAAuC,EAAE,YACxEC,GAAmB,QAAS,mCAAoC,EAAE,YAClEC,GAAW,QAAS,0BAA2B,EAAE,YACjDC,GAAY,QAAS,2BAA4B,EAAE,YACnDC,GAAoB,QAAS,qCAAsC,EACnEC,GAAW,QAAS,gCAAiC,EACrDC,EAAa,QAAS,iCAAkC,EACxDC,EAAW,QAAS,yBAA0B,EAC9CC,EAAS,QAAS,uBAAwB,EAC1CC,EAAiB,IACjBC,EAAiB,IAyBrB,SAASC,GAAUC,EAAMC,EAAU,CAClC,IAAIC,EACJ,GAAK,CAACT,GAAUQ,CAAQ,EACvB,OAAO,IAAI,UAAWL,EAAQ,qEAAsEK,CAAQ,CAAE,EAE/G,GAAKP,EAAYO,EAAS,WAAY,IACrCD,EAAK,UAAYC,EAAQ,UACpB,CAACV,GAAWS,EAAK,SAAU,GAC/B,OAAO,IAAI,UAAWJ,EAAQ,+DAAgE,YAAaI,EAAK,SAAU,CAAE,EAG9H,GAAKN,EAAYO,EAAS,QAAS,IAClCD,EAAK,OAASC,EAAQ,OACjB,CAACb,GAAqBY,EAAK,MAAO,GACtC,OAAO,IAAI,UAAWJ,EAAQ,0EAA2E,SAAUI,EAAK,MAAO,CAAE,EAGnI,GAAKN,EAAYO,EAAS,cAAe,EAAI,CAC5C,GAAK,CAACT,GAAmBS,EAAQ,YAAa,EAC7C,OAAO,IAAI,UAAWL,EAAQ,0EAA2E,eAAgBK,EAAQ,YAAa,CAAE,EAIjJ,GAFAC,EAAOD,EAAQ,aAAc,CAAE,EAC/BD,EAAK,aAAc,CAAE,EAAIE,EACpB,CAACP,EAAUE,EAAgBK,CAAK,EACpC,OAAO,IAAI,UAAWN,EAAQ,sFAAuF,eAAgBC,EAAe,KAAM,MAAO,EAAGK,CAAK,CAAE,EAE5K,GAAKD,EAAQ,aAAa,OAAS,IAC7BC,IAAS,YAAcA,IAAS,gBACpCF,EAAK,aAAc,CAAE,EAAIC,EAAQ,aAAc,CAAE,EAC5C,CAACZ,GAAkBW,EAAK,aAAc,CAAE,CAAE,GAC9C,OAAO,IAAI,UAAWJ,EAAQ,8EAA+E,eAAgBI,EAAK,aAAc,CAAE,CAAE,CAAE,EAIzJ,GAAKC,EAAQ,aAAa,OAAS,GAC7BC,IAAS,eACbF,EAAK,aAAc,CAAE,EAAIC,EAAQ,aAAc,CAAE,EAC5C,CAACX,GAAUU,EAAK,aAAc,CAAE,CAAE,GACtC,OAAO,IAAI,UAAWJ,EAAQ,oEAAqE,eAAgBI,EAAK,aAAc,CAAE,CAAE,CAAE,CAIhJ,CACA,OAAKN,EAAYO,EAAS,MAAO,IAChCD,EAAK,KAAOC,EAAQ,KACf,CAACN,EAAUG,EAAgBE,EAAK,IAAK,GAClC,IAAI,UAAWJ,EAAQ,gFAAiF,OAAQE,EAAe,KAAM,MAAO,EAAGE,EAAK,IAAK,CAAE,EAG7J,IACR,CAKAb,EAAO,QAAUY,KChHjB,IAAAI,EAAAC,EAAA,SAAAC,GAAAC,EAAA,cAsBA,IAAIC,GAAoB,QAAS,oCAAqC,EAAE,YACpEC,GAAe,QAAS,+BAAgC,EACxDC,GAAgB,QAAS,gCAAiC,EAC1DC,GAAc,QAAS,uDAAwD,EAC/EC,EAAS,QAAS,uBAAwB,EAC1CC,GAAQ,IACRC,GAAyB,IACzBC,GAAW,IAqDf,SAASC,GAA0BC,EAAGC,EAAU,CAC/C,IAAIC,EACAC,EACAC,EAEJ,GAAK,CAACb,GAAmBS,CAAE,EAC1B,MAAM,IAAI,UAAWL,EAAQ,4EAA6EK,CAAE,CAAE,EAQ/G,GANAG,EAAO,CACN,UAAa,GACb,OAAU,KACV,aAAgBN,GAAwB,MAAU,MAAM,EACxD,KAAQ,KACT,EACK,UAAU,OAAS,IACvBO,EAAMN,GAAUK,EAAMF,CAAQ,EACzBG,GACJ,MAAMA,EAGR,OAAAF,EAAQ,IAAIN,GAAOI,EAAGG,CAAK,EAG3BT,GAAaW,EAAa,UAAWC,CAAQ,EAEtCD,EA2BP,SAASA,EAAaE,EAAGC,EAAI,CAC5B,GAAK,UAAU,SAAW,EACzB,OAAON,EAAM,aAEd,GAAK,CAACV,GAAce,CAAE,EACrB,MAAM,IAAI,UAAWZ,EAAQ,mFAAoFY,CAAE,CAAE,EAEtH,GAAKC,IAAM,IAAMA,IAAM,EACtB,MAAM,IAAI,UAAWb,EAAQ,0EAA2Ea,CAAE,CAAE,EAE7G,GAAKD,EAAE,MAAO,CAAE,IAAML,EAAM,UAC3B,MAAM,IAAI,UAAWP,EAAQ,wGAAyGO,EAAM,UAAWK,EAAE,MAAO,CAAE,CAAE,CAAE,EAEvK,OAAAL,EAAM,OAAQK,EAAGC,CAAE,EACZN,EAAM,YACd,CA8BA,SAASI,EAASG,EAAGC,EAAO,CAC3B,IAAIC,EACAC,EACJ,GAAK,CAACnB,GAAegB,CAAE,EACtB,MAAM,IAAI,UAAWd,EAAQ,oEAAqEc,CAAE,CAAE,EAGvG,GADAE,EAAKF,EAAE,MACFE,EAAIA,EAAG,OAAO,CAAE,IAAMX,EAC1B,MAAM,IAAI,UAAWL,EAAQ,6GAA8GK,EAAGW,EAAIA,EAAG,OAAO,CAAE,CAAE,CAAE,EAGnK,GADAC,EAAI,QACC,UAAU,OAAS,EAAI,CAC3B,GAAKF,IAAS,eACb,GAAKP,EAAK,OAAS,OAASA,EAAK,OAAS,gBACzC,MAAM,IAAI,MAAOR,EAAQ,0MAA2M,CAAE,MAAO,eAAgB,EAAE,KAAM,MAAO,EAAGQ,EAAK,IAAK,CAAE,UAEjRO,IAAS,SAAWA,IAAS,SACxC,MAAM,IAAI,UAAWf,EAAQ,6HAA8He,CAAK,CAAE,EAEnKE,EAAIF,CACL,CACA,OAAOR,EAAM,QAASO,EAAGG,CAAE,CAC5B,CACD,CAKAtB,EAAO,QAAUS,KCxJjB,IAAIc,GAAO,IAKX,OAAO,QAAUA",
  "names": ["require_model", "__commonJSMin", "exports", "module", "setReadOnly", "setReadOnlyAccessor", "format", "gdot", "gaxpy", "dcopy", "dscal", "max", "exp", "pow", "sigmoid", "Float64Array", "ndarray", "shape2strides", "numel", "vind2bind", "MIN_SCALING_FACTOR", "MIN_SCALE", "LEARNING_RATE_METHODS", "LOSS_METHODS", "Model", "N", "opts", "len", "x", "scale", "s", "w", "buf", "stride", "offset", "v", "y", "eta", "d", "params", "loss", "lambda", "factor", "c", "X", "type", "ndims", "xbuf", "ybuf", "xsh", "ysh", "ord", "ptr", "sxn", "sx", "sy", "ox", "M", "Y", "i", "require_learning_rate_defaults", "__commonJSMin", "exports", "module", "require_learning_rates", "__commonJSMin", "exports", "module", "require_loss_functions", "__commonJSMin", "exports", "module", "require_validate", "__commonJSMin", "exports", "module", "isNonNegativeNumber", "isPositiveNumber", "isNumber", "isBoolean", "isArrayLikeObject", "isObject", "hasOwnProp", "contains", "format", "LEARNING_RATES", "LOSS_FUNCTIONS", "validate", "opts", "options", "name", "require_main", "__commonJSMin", "exports", "module", "isPositiveInteger", "isVectorLike", "isndarrayLike", "setReadOnly", "format", "Model", "LEARNING_RATE_DEFAULTS", "validate", "incrBinaryClassification", "N", "options", "model", "opts", "err", "accumulator", "predict", "x", "y", "X", "type", "sh", "t", "main"]
}
