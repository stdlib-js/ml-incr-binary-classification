{"version":3,"file":"index.mjs","sources":["../lib/model.js","../lib/validate.js","../lib/main.js","../lib/index.js"],"sourcesContent":["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable no-restricted-syntax, no-invalid-this */\n\n'use strict';\n\n// MODULES //\n\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar setReadOnlyAccessor = require( '@stdlib/utils-define-nonenumerable-read-only-accessor' );\nvar format = require( '@stdlib/string-format' );\nvar gdot = require( '@stdlib/blas-base-gdot' ).ndarray;\nvar gaxpy = require( '@stdlib/blas-base-gaxpy' ).ndarray;\nvar dcopy = require( '@stdlib/blas-base-dcopy' );\nvar dscal = require( '@stdlib/blas-base-dscal' );\nvar max = require( '@stdlib/math-base-special-max' );\nvar exp = require( '@stdlib/math-base-special-exp' );\nvar pow = require( '@stdlib/math-base-special-pow' );\nvar sigmoid = require( '@stdlib/math-base-special-expit' );\nvar Float64Array = require( '@stdlib/array-float64' );\nvar ndarray = require( '@stdlib/ndarray-ctor' );\nvar shape2strides = require( '@stdlib/ndarray-base-shape2strides' );\nvar numel = require( '@stdlib/ndarray-base-numel' );\nvar vind2bind = require( '@stdlib/ndarray-base-vind2bind' );\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1.0e-7;\nvar MIN_SCALE = 1.0e-11;\nvar LEARNING_RATE_METHODS = {\n\t'basic': '_basicLearningRate',\n\t'constant': '_constantLearningRate',\n\t'invscaling': '_inverseScalingLearningRate',\n\t'pegasos': '_pegasosLearningRate'\n};\nvar LOSS_METHODS = {\n\t'hinge': '_hingeLoss',\n\t'log': '_logLoss',\n\t'modifiedHuber': '_modifiedHuberLoss',\n\t'perceptron': '_perceptronLoss',\n\t'squaredHinge': '_squaredHingeLoss'\n};\n\n\n// MAIN //\n\n/**\n* Model constructor.\n*\n* ## Notes\n*\n* -   The model (weight vector) implementation is inspired by the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @constructor\n* @param {PositiveInteger} N - number of feature weights (excluding bias/intercept term)\n* @param {Options} opts - model options\n* @param {PositiveNumber} opts.lambda - regularization parameter\n* @param {ArrayLikeObject} opts.learningRate - learning rate function and associated parameters\n* @param {string} opts.loss - loss function\n* @param {boolean} opts.intercept - boolean indicating whether to include an intercept\n* @returns {Model} model\n*/\nfunction Model( N, opts ) {\n\tvar len;\n\n\t// Set internal properties:\n\tthis._N = N;\n\tthis._opts = opts;\n\n\tthis._scaleFactor = 1.0;\n\tthis._t = 0; // iteration counter (i.e., number of updates)\n\n\t// Determine the learning rate function:\n\tthis._learningRateMethod = LEARNING_RATE_METHODS[ opts.learningRate[ 0 ] ];\n\n\t// Determine the loss function:\n\tthis._lossMethod = LOSS_METHODS[ opts.loss ];\n\n\t// Determine the number of model coefficients:\n\tlen = N;\n\tif ( opts.intercept ) {\n\t\tlen += 1;\n\t}\n\t// Initialize a model weight vector with all weights set to zero:\n\tthis._weights = new Float64Array( len );\n\n\t// Initialize model coefficients to zero:\n\tthis._coefficients = new ndarray( 'float64', new Float64Array( len ), [ len ], [ 1 ], 0, 'row-major' );\n\n\treturn this;\n}\n\n/**\n* Adds a provided input vector to the model weight vector.\n*\n* @private\n* @name _add\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - input vector\n* @param {number} scale - scale factor\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_add', function add( x, scale ) {\n\tvar s = scale / this._scaleFactor;\n\tvar w = this._weights;\n\n\t// Scale `x` and add to the model weight vector:\n\tgaxpy( x.shape[ 0 ], s, x.data, x.strides[ 0 ], x.offset, w, 1, 0 );\n\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this._opts.intercept ) {\n\t\tw[ this._N ] += s;\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate.\n*\n* ## Notes\n*\n* -   This learning rate function is based on the learning rate function of the same name in the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @name _basicLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_basicLearningRate', function basic() {\n\treturn 10.0 / ( 10.0+this._t );\n});\n\n/**\n* Returns a constant learning rate.\n*\n* @private\n* @name _constantLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_constantLearningRate', function constant() {\n\treturn this._opts.learningRate[ 1 ];\n});\n\n/**\n* Calculates the dot product of the model weight vector and a provided vector `x`.\n*\n* @private\n* @name _dot\n* @memberof Model.prototype\n* @type {Function}\n* @param {NumericArray} buf - ndarray data buffer\n* @param {integer} stride - stride\n* @param {NonNegativeInteger} offset - index offset\n* @returns {number} dot product\n*/\nsetReadOnly( Model.prototype, '_dot', function dot( buf, stride, offset ) {\n\tvar v = gdot( this._N, this._weights, 1, 0, buf, stride, offset );\n\tif ( this._opts.intercept ) {\n\t\tv += this._weights[ this._N ];\n\t}\n\tv *= this._scaleFactor;\n\treturn v;\n});\n\n/**\n* Updates the model weight vector using the hinge loss function.\n*\n* ## Notes\n*\n* -   The hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _hingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_hingeLoss', function hingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) < 1.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate according to an inverse scaling formula.\n*\n* ## Notes\n*\n* -   The inverse scaling formula is defined as\n*\n*     ```tex\n*     \\eta = \\frac{\\eta_0}{t^{k}}\n*     ```\n*\n*     where \\\\(\\eta_0\\\\) is an initial learning rate, \\\\(t\\\\) is the current iteration, and \\\\(k\\\\) is an exponent controlling how quickly the learning rate decreases.\n*\n* @private\n* @name _inverseScalingLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_inverseScalingLearningRate', function invscaling() {\n\tvar params = this._opts.learningRate;\n\treturn params[ 1 ] / pow( this._t, params[ 2 ] );\n});\n\n/**\n* Updates the model weight vector using the log loss function.\n*\n* ## Notes\n*\n* -   The log loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\ln( 1 + \\exp( -y\\,f(x) ) )\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n*\n* @private\n* @name _logLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_logLoss', function logLoss( x, y ) {\n\tvar loss;\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tloss = y / ( 1.0 + exp( y*d ) );\n\tthis._add( x, eta*loss );\n\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the modified Huber loss function.\n*\n* ## Notes\n*\n* -   The modified Huber loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\begin{cases}\n*       \\max(0, 1 - y\\,f(x))^2 & \\textrm{for}\\,\\,y\\,f(x) \\geq -1\\\\\n*       -4y\\,f(x) & \\textrm{otherwise}\n*     \\end{cases}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* ## References\n*\n* -   Zhang, Tong. 2004. \"Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms.\" In _Proceedings of the Twenty-First International Conference on Machine Learning_, 116. New York, NY, USA: Association for Computing Machinery. doi:[10.1145/1015330.1015332][@zhang:2004a].\n*\n* [@zhang:2004a]: https://doi.org/10.1145/1015330.1015332\n*\n* @private\n* @name _modifiedHuberLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_modifiedHuberLoss', function modifiedHuber( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < -1.0 ) {\n\t\tthis._add( x, 4.0*eta*y );\n\t} else {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate using Pegasos.\n*\n* ## References\n*\n* -   Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. \"Pegasos: primal estimated sub-gradient solver for SVM.\" _Mathematical Programming_ 127 (1): 3–30. doi:[10.1007/s10107-010-0420-4][@shalevshwartz:2011a].\n*\n* [@shalevshwartz:2011a]: https://doi.org/10.1007/s10107-010-0420-4\n*\n* @private\n* @name _pegasos\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_pegasosLearningRate', function pegasos() {\n\treturn 1.0 / ( this._opts.lambda*this._t );\n});\n\n/**\n* Updates the model weight vector using the perceptron loss function.\n*\n* ## Notes\n*\n* -   The perceptron loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max(0, -y\\,f(x))\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* -   The perceptron loss function is equivalent to the hinge loss function without a margin.\n*\n* -   The perceptron loss function does not update the model weight vector when the response is correctly classified.\n*\n* ## References\n*\n* -   Rosenblatt, Frank. 1957. \"The Perceptron–a perceiving and recognizing automaton.\" 85-460-1. Buffalo, NY, USA: Cornell Aeronautical Laboratory.\n*\n* @private\n* @name _perceptronLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_perceptronLoss', function perceptron( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) <= 0.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Performs L2 regularization of the model weights.\n*\n* @private\n* @name _regularize\n* @memberof Model.prototype\n* @type {Function}\n* @param {PositiveNumber} eta - learning rate\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_regularize', function regularize( eta ) {\n\tvar lambda = this._opts.lambda;\n\tif ( lambda <= 0.0 ) {\n\t\treturn this;\n\t}\n\tthis._scale( max( 1.0-( eta*lambda ), MIN_SCALING_FACTOR ) );\n\treturn this;\n});\n\n/**\n* Scale the model weight vector by a provided scaling factor.\n*\n* @private\n* @name _scale\n* @memberof Model.prototype\n* @type {Function}\n* @param {number} factor - scaling factor\n* @throws {RangeError} scaling factor must be a positive number\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_scale', function scale( factor ) {\n\tvar s;\n\tif ( factor <= 0.0 ) {\n\t\tthrow new RangeError( format( 'invalid argument. Attempting to scale a weight vector by a nonpositive value. This is likely due to too large a value of `eta*lambda`. Value: `%f`.', factor ) );\n\t}\n\t// Check whether we need to scale the weight vector to unity in order to avoid numerical issues...\n\ts = this._scaleFactor;\n\tif ( s < MIN_SCALE ) {\n\t\t// Note: we only scale/shrink the feature weights, not the intercept...\n\t\tdscal( this._N, s, this._weights, 1 );\n\t\tthis._scaleFactor = 1.0;\n\t}\n\tthis._scaleFactor *= factor;\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the squared hinge loss function.\n*\n* ## Notes\n*\n* -   The squared hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}^2\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _squaredHingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_squaredHingeLoss', function squaredHingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < 1.0 ) {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Returns the model coefficients.\n*\n* @private\n* @name coefficients\n* @memberof Model.prototype\n* @type {Function}\n* @returns {ndarray} model coefficients\n*/\nsetReadOnlyAccessor( Model.prototype, 'coefficients', function coefficients() {\n\tvar c = this._coefficients.data;\n\tvar w = this._weights;\n\tdcopy( w.length, w, 1, c, 1 );\n\tdscal( this._N, this._scaleFactor, c, 1 );\n\treturn this._coefficients;\n});\n\n/**\n* Returns the number of model features.\n*\n* @private\n* @name nfeatures\n* @memberof Model.prototype\n* @type {PositiveInteger}\n*/\nsetReadOnlyAccessor( Model.prototype, 'nfeatures', function nfeatures() {\n\treturn this._N;\n});\n\n/**\n* Predicts the response value for one or more observation vectors `X`.\n*\n* @private\n* @name predict\n* @memberof Model.prototype\n* @type {Function}\n* @param {ndarray} X - feature vector\n* @param {string} type - prediction type\n* @returns {ndarray} ndarray containing response values\n*/\nsetReadOnly( Model.prototype, 'predict', function predict( X, type ) {\n\tvar ndims;\n\tvar xbuf;\n\tvar ybuf;\n\tvar xsh;\n\tvar ysh;\n\tvar ord;\n\tvar ptr;\n\tvar sxn;\n\tvar sx;\n\tvar sy;\n\tvar ox;\n\tvar M;\n\tvar N;\n\tvar Y;\n\tvar v;\n\tvar i;\n\n\t// Cache input array properties in case of lazy evaluation:\n\txbuf = X.data;\n\txsh = X.shape;\n\tsx = X.strides;\n\tox = X.offset;\n\tord = X.order;\n\n\tndims = xsh.length - 1;\n\n\t// The output array shape is the same as the input array shape without the last dimension (i.e., the number of dimensions is reduced by one)...\n\tysh = [];\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tysh.push( xsh[ i ] );\n\t}\n\t// Create an output array...\n\tif ( ndims === 0 ) {\n\t\tM = 1;\n\t\tybuf = new Float64Array( 1 );\n\t\tsy = [ 0 ];\n\t} else {\n\t\tM = numel( ysh );\n\t\tybuf = new Float64Array( M );\n\t\tsy = shape2strides( ysh, ord );\n\t}\n\tY = new ndarray( 'int8', ybuf, ysh, sy, 0, ord );\n\n\t// Loop over all observation vectors...\n\tN = this._N; // number of features (i.e., size of last `X` dimension)\n\tsxn = sx[ ndims ]; // stride of the last `X` dimension\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Compute the index offset into the underlying data buffer pointing to the start of the current observation vector:\n\t\tptr = vind2bind( xsh, sx, ox, ord, i*N, 'throw' );\n\n\t\t// Compute the dot product of the current observation vector with the model weight vector:\n\t\tv = this._dot( xbuf, sxn, ptr );\n\n\t\t// Determine the output value:\n\t\tif ( type === 'label' ) {\n\t\t\tv = ( v > 0 ) ? 1 : -1;\n\t\t} else if ( type === 'probability' ) {\n\t\t\tv = sigmoid( v );\n\t\t} // else type === 'linear' (i.e., linear predictor)\n\n\t\t// Set the element in the output array:\n\t\tif ( ndims === 0 ) {\n\t\t\tY.iset( v );\n\t\t} else {\n\t\t\tY.iset( i, v );\n\t\t}\n\t}\n\treturn Y;\n});\n\n/**\n* Updates a model given a provided observation vector and response value.\n*\n* @private\n* @name update\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, 'update', function update( x, y ) {\n\tthis._t += 1;\n\treturn this[ this._lossMethod ]( x, y );\n});\n\n\n// EXPORTS //\n\nmodule.exports = Model;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar isNonNegativeNumber = require( '@stdlib/assert-is-nonnegative-number' ).isPrimitive;\nvar isPositiveNumber = require( '@stdlib/assert-is-positive-number' ).isPrimitive;\nvar isNumber = require( '@stdlib/assert-is-number' ).isPrimitive;\nvar isBoolean = require( '@stdlib/assert-is-boolean' ).isPrimitive;\nvar isArrayLikeObject = require( '@stdlib/assert-is-array-like-object' );\nvar isObject = require( '@stdlib/assert-is-plain-object' );\nvar hasOwnProp = require( '@stdlib/assert-has-own-property' );\nvar contains = require( '@stdlib/assert-contains' );\nvar format = require( '@stdlib/string-format' );\nvar LEARNING_RATES = require( './learning_rates.json' );\nvar LOSS_FUNCTIONS = require( './loss_functions.json' );\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate] - learning rate function\n* @param {string} [options.loss] - loss function\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tvar name;\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( 'invalid argument. Options argument must be an object. Value: `%s`.', options ) );\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a boolean. Option: `%s`.', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be a nonnegative number. Option: `%s`.', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\tif ( !isArrayLikeObject( options.learningRate ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be an array-like object. Option: `%s`.', 'learningRate', options.learningRate ) );\n\t\t}\n\t\tname = options.learningRate[ 0 ];\n\t\topts.learningRate[ 0 ] = name;\n\t\tif ( !contains( LEARNING_RATES, name ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. First `%s` option must be one of the following: \"%s\". Option: `%s`.', 'learningRate', LEARNING_RATES.join( '\", \"' ), name ) );\n\t\t}\n\t\tif ( options.learningRate.length > 1 ) {\n\t\t\tif ( name === 'constant' || name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 1 ] = options.learningRate[ 1 ];\n\t\t\t\tif ( !isPositiveNumber( opts.learningRate[ 1 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Second `%s` option must be a positive number. Option: `%f`.', 'learningRate', opts.learningRate[ 1 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ( options.learningRate.length > 2 ) {\n\t\t\tif ( name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 2 ] = options.learningRate[ 2 ];\n\t\t\t\tif ( !isNumber( opts.learningRate[ 2 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Third `%s` option must be a number. Option: `%f`.', 'learningRate', opts.learningRate[ 2 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !contains( LOSS_FUNCTIONS, opts.loss ) ) {\n\t\t\treturn new TypeError( format( 'invalid option. `%s` option must be one of the following: \"%s\". Option: `%s`.', 'loss', LOSS_FUNCTIONS.join( '\", \"' ), opts.loss ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nmodule.exports = validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nvar isPositiveInteger = require( '@stdlib/assert-is-positive-integer' ).isPrimitive;\nvar isVectorLike = require( '@stdlib/assert-is-vector-like' );\nvar isndarrayLike = require( '@stdlib/assert-is-ndarray-like' );\nvar setReadOnly = require( '@stdlib/utils-define-nonenumerable-read-only-property' );\nvar format = require( '@stdlib/string-format' );\nvar Model = require( './model.js' );\nvar LEARNING_RATE_DEFAULTS = require( './learning_rate_defaults.json' );\nvar validate = require( './validate.js' );\n\n\n// MAIN //\n\n/**\n* Returns an accumulator function which incrementally performs binary classification using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* -   The sub-gradient of the loss function is estimated for each datum and the classification model is updated incrementally, with a decreasing learning rate and regularization of model feature weights using L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3–30. doi:10.1007/s10107-010-0420-4\n*\n* @param {PositiveInteger} N - number of features\n* @param {Options} [options] - options object\n* @param {PositiveNumber} [options.lambda=1.0e-3] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate=['basic']] - learning rate function and associated parameters (one of `basic`, `constant`, or `pegasos`)\n* @param {string} [options.loss='log'] - loss function (one of `hinge`, `log`, `modifiedHuber`, `perceptron`, or `squaredHinge`)\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Function} accumulator\n*\n* @example\n* var Float64Array = require( '@stdlib/array-float64' );\n* var array = require( '@stdlib/ndarray-array' );\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\nfunction incrBinaryClassification( N, options ) {\n\tvar model;\n\tvar opts;\n\tvar err;\n\n\tif ( !isPositiveInteger( N ) ) {\n\t\tthrow new TypeError( format( 'invalid argument. First argument must be a positive integer. Value: `%s`.', N ) );\n\t}\n\topts = {\n\t\t'intercept': true,\n\t\t'lambda': 1.0e-4,\n\t\t'learningRate': LEARNING_RATE_DEFAULTS[ 'basic' ].slice(),\n\t\t'loss': 'log'\n\t};\n\tif ( arguments.length > 1 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\tmodel = new Model( N, opts );\n\n\t// Attach methods to the accumulator:\n\tsetReadOnly( accumulator, 'predict', predict );\n\n\treturn accumulator;\n\n\t/**\n\t* If provided a feature vector and response value, the accumulator function updates a binary classification model; otherwise, the accumulator function returns the current binary classification model coefficients.\n\t*\n\t* @private\n\t* @param {VectorLike} x - feature vector\n\t* @param {integer} y - response value\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray whose length matches the number of model features\n\t* @throws {TypeError} second argument must be either `+1` or `-1`\n\t* @returns {ndarray} one-dimensional ndarray containing model coefficients\n\t*\n\t* @example\n\t* var Float64Array = require( '@stdlib/array-float64' );\n\t* var array = require( '@stdlib/ndarray-array' );\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Update the model:\n\t* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n\t* var coefs = accumulator( x, 1 );\n\t* // returns <ndarray>\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tif ( arguments.length === 0 ) {\n\t\t\treturn model.coefficients;\n\t\t}\n\t\tif ( !isVectorLike( x ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray. Value: `%s`.', x ) );\n\t\t}\n\t\tif ( y !== -1 && y !== 1 ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be either +1 or -1. Value: `%s`.', y ) );\n\t\t}\n\t\tif ( x.shape[ 0 ] !== model.nfeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray of length `%u`. Actual length: `%u`.', model.nfeatures, x.shape[ 0 ] ) );\n\t\t}\n\t\tmodel.update( x, y );\n\t\treturn model.coefficients;\n\t}\n\n\t/**\n\t* Predicts the response value for one or more observation vectors `X`.\n\t*\n\t* @private\n\t* @param {ndarrayLike} X - ndarray (of size `(...,N)`) containing observation vectors\n\t* @param {string} [type=\"label\"] - prediction type (either `label`, `probability`, or `linear`)\n\t* @throws {TypeError} first argument must be an ndarray\n\t* @throws {TypeError} first argument must be an ndarray whose last dimension matches the number of model features\n\t* @throws {TypeError} second argument must be a recognized/supported prediction \"type\"\n\t* @throws {Error} second argument must be compatible with the model loss function\n\t* @returns {ndarray} ndarray (of size `(...)`) containing response values\n\t*\n\t* @example\n\t* var Float64Array = require( '@stdlib/array-float64' );\n\t* var array = require( '@stdlib/ndarray-array' );\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Create a new observation vector:\n\t* var x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n\t*\n\t* // Predict the response value:\n\t* var yhat = accumulator.predict( x );\n\t* // returns <ndarray>\n\t*/\n\tfunction predict( X, type ) {\n\t\tvar sh;\n\t\tvar t;\n\t\tif ( !isndarrayLike( X ) ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray. Value: `%s`.', X ) );\n\t\t}\n\t\tsh = X.shape;\n\t\tif ( sh[ sh.length-1 ] !== N ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray whose last dimension is of size `%u`. Actual size: `%u`.', N, sh[ sh.length-1 ] ) );\n\t\t}\n\t\tt = 'label';\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( type === 'probability' ) {\n\t\t\t\tif ( opts.loss !== 'log' && opts.loss !== 'modifiedHuber' ) {\n\t\t\t\t\tthrow new Error( format( 'invalid argument. Second argument is incompatible with model loss function. Probability predictions are only supported when the loss function is either `log` or `modifiedHuber`. Model loss function: `%s`.', opts.loss ) );\n\t\t\t\t}\n\t\t\t} else if ( type !== 'label' && type !== 'linear' ) {\n\t\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be a string value equal to either \"label\", \"probability\", or \"linear\". Value: `%s`.', type ) );\n\t\t\t}\n\t\t\tt = type;\n\t\t}\n\t\treturn model.predict( X, t );\n\t}\n}\n\n\n// EXPORTS //\n\nmodule.exports = incrBinaryClassification;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n/**\n* Incrementally perform binary classification using stochastic gradient descent (SGD).\n*\n* @module @stdlib/ml-incr-binary-classification\n*\n* @example\n* var Float64Array = require( '@stdlib/array-float64' );\n* var array = require( '@stdlib/ndarray-array' );\n* var incrBinaryClassification = require( '@stdlib/ml-incr-binary-classification' );\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\n\n// MODULES //\n\nvar incrBinaryClassification = require( './main.js' );\n\n\n// EXPORTS //\n\nmodule.exports = incrBinaryClassification;\n"],"names":["setReadOnly","require$$0","setReadOnlyAccessor","require$$1","format","require$$2","gdot","require$$3","ndarray","gaxpy","require$$4","dcopy","require$$5","dscal","require$$6","max","require$$7","exp","require$$8","pow","require$$9","sigmoid","require$$10","Float64Array","require$$11","require$$12","shape2strides","require$$13","numel","require$$14","vind2bind","require$$15","LEARNING_RATE_METHODS","basic","constant","invscaling","pegasos","LOSS_METHODS","hinge","log","modifiedHuber","perceptron","squaredHinge","Model","N","opts","len","this","_N","_opts","_scaleFactor","_t","_learningRateMethod","learningRate","_lossMethod","loss","intercept","_weights","_coefficients","prototype","x","scale","s","w","shape","data","strides","offset","buf","stride","v","y","eta","_regularize","_dot","_add","params","d","lambda","_scale","factor","RangeError","c","length","X","type","ndims","xbuf","ybuf","xsh","ysh","ord","ptr","sxn","sx","sy","ox","M","Y","i","order","push","iset","model","isNonNegativeNumber","isPrimitive","isPositiveNumber","isNumber","isBoolean","isArrayLikeObject","isObject","hasOwnProp","contains","LEARNING_RATES","LOSS_FUNCTIONS","validate_1","options","name","TypeError","join","isPositiveInteger","isVectorLike","isndarrayLike","LEARNING_RATE_DEFAULTS","validate","main","err","slice","arguments","accumulator","predict","coefficients","nfeatures","update","sh","t","Error","lib"],"mappings":";;i3EAwBA,IAAIA,EAAcC,EACdC,EAAsBC,EACtBC,EAASC,EACTC,EAAOC,EAAoCC,QAC3CC,EAAQC,EAAqCF,QAC7CG,EAAQC,EACRC,EAAQC,EACRC,EAAMC,EACNC,EAAMC,EACNC,EAAMC,EACNC,EAAUC,EACVC,EAAeC,EACfhB,EAAUiB,EACVC,EAAgBC,EAChBC,EAAQC,EACRC,EAAYC,EAOZC,EAAwB,CAC3BC,MAAS,qBACTC,SAAY,wBACZC,WAAc,8BACdC,QAAW,wBAERC,EAAe,CAClBC,MAAS,aACTC,IAAO,WACPC,cAAiB,qBACjBC,WAAc,kBACdC,aAAgB,qBAyBjB,SAASC,EAAOC,EAAGC,GAClB,IAAIC,EA0BJ,OAvBAC,KAAKC,GAAKJ,EACVG,KAAKE,MAAQJ,EAEbE,KAAKG,aAAe,EACpBH,KAAKI,GAAK,EAGVJ,KAAKK,oBAAsBpB,EAAuBa,EAAKQ,aAAc,IAGrEN,KAAKO,YAAcjB,EAAcQ,EAAKU,MAGtCT,EAAMF,EACDC,EAAKW,YACTV,GAAO,GAGRC,KAAKU,SAAW,IAAIlC,EAAcuB,GAGlCC,KAAKW,cAAgB,IAAIlD,EAAS,UAAW,IAAIe,EAAcuB,GAAO,CAAEA,GAAO,CAAE,GAAK,EAAG,aAElFC,KAcR/C,EAAa2C,EAAMgB,UAAW,QAAQ,SAAcC,EAAGC,GACtD,IAAIC,EAAID,EAAQd,KAAKG,aACjBa,EAAIhB,KAAKU,SASb,OANAhD,EAAOmD,EAAEI,MAAO,GAAKF,EAAGF,EAAEK,KAAML,EAAEM,QAAS,GAAKN,EAAEO,OAAQJ,EAAG,EAAG,GAG3DhB,KAAKE,MAAMO,YACfO,EAAGhB,KAAKC,KAAQc,GAEVf,QAkBR/C,EAAa2C,EAAMgB,UAAW,sBAAsB,WACnD,OAAO,IAAS,GAAKZ,KAAKI,OAY3BnD,EAAa2C,EAAMgB,UAAW,yBAAyB,WACtD,OAAOZ,KAAKE,MAAMI,aAAc,MAejCrD,EAAa2C,EAAMgB,UAAW,QAAQ,SAAcS,EAAKC,EAAQF,GAChE,IAAIG,EAAIhE,EAAMyC,KAAKC,GAAID,KAAKU,SAAU,EAAG,EAAGW,EAAKC,EAAQF,GAKzD,OAJKpB,KAAKE,MAAMO,YACfc,GAAKvB,KAAKU,SAAUV,KAAKC,KAE1BsB,GAAKvB,KAAKG,gBA+BXlD,EAAa2C,EAAMgB,UAAW,cAAc,SAAoBC,EAAGW,GAClE,IAAIC,EAUJ,OAPAA,EAAMzB,KAAMA,KAAKK,uBACjBL,KAAK0B,YAAaD,GAGXD,EADHxB,KAAK2B,KAAMd,EAAEK,KAAML,EAAEM,QAAS,GAAKN,EAAEO,QAC1B,GACdpB,KAAK4B,KAAMf,EAAGW,EAAEC,GAEVzB,QAsBR/C,EAAa2C,EAAMgB,UAAW,+BAA+B,WAC5D,IAAIiB,EAAS7B,KAAKE,MAAMI,aACxB,OAAOuB,EAAQ,GAAMzD,EAAK4B,KAAKI,GAAIyB,EAAQ,OA+B5C5E,EAAa2C,EAAMgB,UAAW,YAAY,SAAkBC,EAAGW,GAC9D,IAAIhB,EACAiB,EACAK,EASJ,OAPAL,EAAMzB,KAAMA,KAAKK,uBACjBL,KAAK0B,YAAaD,GAElBK,EAAI9B,KAAK2B,KAAMd,EAAEK,KAAML,EAAEM,QAAS,GAAKN,EAAEO,QACzCZ,EAAOgB,GAAM,EAAMtD,EAAKsD,EAAEM,IAC1B9B,KAAK4B,KAAMf,EAAGY,EAAIjB,GAEXR,QAuCR/C,EAAa2C,EAAMgB,UAAW,sBAAsB,SAAwBC,EAAGW,GAC9E,IAAIC,EACAK,EAWJ,OATAL,EAAMzB,KAAMA,KAAKK,uBACjBL,KAAK0B,YAAaD,IAElBK,EAAIN,EAAIxB,KAAK2B,KAAMd,EAAEK,KAAML,EAAEM,QAAS,GAAKN,EAAEO,UACnC,EACTpB,KAAK4B,KAAMf,EAAG,EAAIY,EAAID,GAEtBxB,KAAK4B,KAAMf,EAAGY,GAAMD,EAAGM,EAAEN,IAEnBxB,QAkBR/C,EAAa2C,EAAMgB,UAAW,wBAAwB,WACrD,OAAO,GAAQZ,KAAKE,MAAM6B,OAAO/B,KAAKI,OAsCvCnD,EAAa2C,EAAMgB,UAAW,mBAAmB,SAAqBC,EAAGW,GACxE,IAAIC,EAUJ,OAPAA,EAAMzB,KAAMA,KAAKK,uBACjBL,KAAK0B,YAAaD,GAGXD,EADHxB,KAAK2B,KAAMd,EAAEK,KAAML,EAAEM,QAAS,GAAKN,EAAEO,SACzB,GACfpB,KAAK4B,KAAMf,EAAGW,EAAEC,GAEVzB,QAaR/C,EAAa2C,EAAMgB,UAAW,eAAe,SAAqBa,GACjE,IAAIM,EAAS/B,KAAKE,MAAM6B,OACxB,OAAKA,GAAU,GAGf/B,KAAKgC,OAAQhE,EAAK,EAAMyD,EAAIM,EArYJ,OAmYhB/B,QAiBT/C,EAAa2C,EAAMgB,UAAW,UAAU,SAAgBqB,GACvD,IAAIlB,EACJ,GAAKkB,GAAU,EACd,MAAM,IAAIC,WAAY7E,EAAQ,sJAAuJ4E,IAUtL,OAPAlB,EAAIf,KAAKG,cAzZM,QA4ZdrC,EAAOkC,KAAKC,GAAIc,EAAGf,KAAKU,SAAU,GAClCV,KAAKG,aAAe,GAErBH,KAAKG,cAAgB8B,EACdjC,QA8BR/C,EAAa2C,EAAMgB,UAAW,qBAAqB,SAA2BC,EAAGW,GAChF,IAAIC,EACAK,EASJ,OAPAL,EAAMzB,KAAMA,KAAKK,uBACjBL,KAAK0B,YAAaD,IAElBK,EAAIN,EAAIxB,KAAK2B,KAAMd,EAAEK,KAAML,EAAEM,QAAS,GAAKN,EAAEO,SACpC,GACRpB,KAAK4B,KAAMf,EAAGY,GAAMD,EAAGM,EAAEN,IAEnBxB,QAYR7C,EAAqByC,EAAMgB,UAAW,gBAAgB,WACrD,IAAIuB,EAAInC,KAAKW,cAAcO,KACvBF,EAAIhB,KAAKU,SAGb,OAFA9C,EAAOoD,EAAEoB,OAAQpB,EAAG,EAAGmB,EAAG,GAC1BrE,EAAOkC,KAAKC,GAAID,KAAKG,aAAcgC,EAAG,GAC/BnC,KAAKW,iBAWbxD,EAAqByC,EAAMgB,UAAW,aAAa,WAClD,OAAOZ,KAAKC,MAcbhD,EAAa2C,EAAMgB,UAAW,WAAW,SAAkByB,EAAGC,GAC7D,IAAIC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACArD,EACAsD,EACA5B,EACA6B,EAaJ,IAVAZ,EAAOH,EAAEnB,KACTwB,EAAML,EAAEpB,MACR8B,EAAKV,EAAElB,QACP8B,EAAKZ,EAAEjB,OACPwB,EAAMP,EAAEgB,MAERd,EAAQG,EAAIN,OAAS,EAGrBO,EAAM,GACAS,EAAI,EAAGA,EAAIb,EAAOa,IACvBT,EAAIW,KAAMZ,EAAKU,IAiBhB,IAde,IAAVb,GACJW,EAAI,EACJT,EAAO,IAAIjE,EAAc,GACzBwE,EAAK,CAAE,KAEPE,EAAIrE,EAAO8D,GACXF,EAAO,IAAIjE,EAAc0E,GACzBF,EAAKrE,EAAegE,EAAKC,IAE1BO,EAAI,IAAI1F,EAAS,OAAQgF,EAAME,EAAKK,EAAI,EAAGJ,GAG3C/C,EAAIG,KAAKC,GACT6C,EAAMC,EAAIR,GACJa,EAAI,EAAGA,EAAIF,EAAGE,IAEnBP,EAAM9D,EAAW2D,EAAKK,EAAIE,EAAIL,EAAKQ,EAAEvD,EAAG,SAGxC0B,EAAIvB,KAAK2B,KAAMa,EAAMM,EAAKD,GAGZ,UAATP,EACJf,EAAMA,EAAI,EAAM,GAAK,EACD,gBAATe,IACXf,EAAIjD,EAASiD,IAIC,IAAVgB,EACJY,EAAEI,KAAMhC,GAER4B,EAAEI,KAAMH,EAAG7B,GAGb,OAAO4B,KAcRlG,EAAa2C,EAAMgB,UAAW,UAAU,SAAiBC,EAAGW,GAE3D,OADAxB,KAAKI,IAAM,EACJJ,KAAMA,KAAKO,aAAeM,EAAGW,MAMrC,IAAAgC,EAAiB5D,qGCrmBb6D,EAAsBvG,EAAkDwG,YACxEC,EAAmBvG,EAA+CsG,YAClEE,EAAWtG,EAAsCoG,YACjDG,EAAYrG,EAAuCkG,YACnDI,EAAoBnG,EACpBoG,EAAWlG,EACXmG,GAAajG,EACbkG,GAAWhG,EACXZ,GAASc,EACT+F,+CACAC,+DAgFJ,IAAAC,GAvDA,SAAmBtE,EAAMuE,GACxB,IAAIC,EACJ,IAAMP,EAAUM,GACf,OAAO,IAAIE,UAAWlH,GAAQ,qEAAsEgH,IAErG,GAAKL,GAAYK,EAAS,eACzBvE,EAAKW,UAAY4D,EAAQ5D,WACnBoD,EAAW/D,EAAKW,YACrB,OAAO,IAAI8D,UAAWlH,GAAQ,+DAAgE,YAAayC,EAAKW,YAGlH,GAAKuD,GAAYK,EAAS,YACzBvE,EAAKiC,OAASsC,EAAQtC,QAChB0B,EAAqB3D,EAAKiC,SAC/B,OAAO,IAAIwC,UAAWlH,GAAQ,0EAA2E,SAAUyC,EAAKiC,SAG1H,GAAKiC,GAAYK,EAAS,gBAAmB,CAC5C,IAAMP,EAAmBO,EAAQ/D,cAChC,OAAO,IAAIiE,UAAWlH,GAAQ,0EAA2E,eAAgBgH,EAAQ/D,eAIlI,GAFAgE,EAAOD,EAAQ/D,aAAc,GAC7BR,EAAKQ,aAAc,GAAMgE,GACnBL,GAAUC,GAAgBI,GAC/B,OAAO,IAAIC,UAAWlH,GAAQ,sFAAuF,eAAgB6G,GAAeM,KAAM,QAAUF,IAErK,GAAKD,EAAQ/D,aAAa8B,OAAS,IACpB,aAATkC,GAAgC,eAATA,KAC3BxE,EAAKQ,aAAc,GAAM+D,EAAQ/D,aAAc,IACzCqD,EAAkB7D,EAAKQ,aAAc,KAC1C,OAAO,IAAIiE,UAAWlH,GAAQ,8EAA+E,eAAgByC,EAAKQ,aAAc,KAInJ,GAAK+D,EAAQ/D,aAAa8B,OAAS,GACpB,eAATkC,IACJxE,EAAKQ,aAAc,GAAM+D,EAAQ/D,aAAc,IACzCsD,EAAU9D,EAAKQ,aAAc,KAClC,OAAO,IAAIiE,UAAWlH,GAAQ,oEAAqE,eAAgByC,EAAKQ,aAAc,KAK1I,OAAK0D,GAAYK,EAAS,UACzBvE,EAAKU,KAAO6D,EAAQ7D,MACdyD,GAAUE,GAAgBrE,EAAKU,OAC7B,IAAI+D,UAAWlH,GAAQ,gFAAiF,OAAQ8G,GAAeK,KAAM,QAAU1E,EAAKU,OAGtJ,MCpFJiE,GAAoBvH,EAAgDwG,YACpEgB,GAAetH,EACfuH,GAAgBrH,EAChBL,GAAcO,EACdH,GAASM,EACTiC,GAAQ/B,EACR+G,GAAyB7G,EACzB8G,GAAW5G,GAkLf,IAAA6G,GA7HA,SAAmCjF,EAAGwE,GACrC,IAAIb,EACA1D,EACAiF,EAEJ,IAAMN,GAAmB5E,GACxB,MAAM,IAAI0E,UAAWlH,GAAQ,4EAA6EwC,IAQ3G,GANAC,EAAO,CACNW,WAAa,EACbsB,OAAU,KACVzB,aAAgBsE,GAAiC,MAACI,QAClDxE,KAAQ,OAEJyE,UAAU7C,OAAS,IACvB2C,EAAMF,GAAU/E,EAAMuE,IAErB,MAAMU,EAQR,OALAvB,EAAQ,IAAI5D,GAAOC,EAAGC,GAGtB7C,GAAaiI,EAAa,UAAWC,GAE9BD,EA2BP,SAASA,EAAarE,EAAGW,GACxB,GAA0B,IAArByD,UAAU7C,OACd,OAAOoB,EAAM4B,aAEd,IAAMV,GAAc7D,GACnB,MAAM,IAAI0D,UAAWlH,GAAQ,mFAAoFwD,IAElH,IAAY,IAAPW,GAAkB,IAANA,EAChB,MAAM,IAAI+C,UAAWlH,GAAQ,0EAA2EmE,IAEzG,GAAKX,EAAEI,MAAO,KAAQuC,EAAM6B,UAC3B,MAAM,IAAId,UAAWlH,GAAQ,0GAA2GmG,EAAM6B,UAAWxE,EAAEI,MAAO,KAGnK,OADAuC,EAAM8B,OAAQzE,EAAGW,GACVgC,EAAM4B,aA+Bd,SAASD,EAAS9C,EAAGC,GACpB,IAAIiD,EACAC,EACJ,IAAMb,GAAetC,GACpB,MAAM,IAAIkC,UAAWlH,GAAQ,oEAAqEgF,IAGnG,IADAkD,EAAKlD,EAAEpB,OACEsE,EAAGnD,OAAO,KAAQvC,EAC1B,MAAM,IAAI0E,UAAWlH,GAAQ,+GAAgHwC,EAAG0F,EAAIA,EAAGnD,OAAO,KAG/J,GADAoD,EAAI,QACCP,UAAU7C,OAAS,EAAI,CAC3B,GAAc,gBAATE,GACJ,GAAmB,QAAdxC,EAAKU,MAAgC,kBAAdV,EAAKU,KAChC,MAAM,IAAIiF,MAAOpI,GAAQ,+MAAgNyC,EAAKU,YAEzO,GAAc,UAAT8B,GAA6B,WAATA,EAC/B,MAAM,IAAIiC,UAAWlH,GAAQ,6HAA8HiF,IAE5JkD,EAAIlD,EAEL,OAAOkB,EAAM2B,QAAS9C,EAAGmD,KC5I3BE,GAL+BxI"}