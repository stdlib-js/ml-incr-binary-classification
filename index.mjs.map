{"version":3,"file":"index.mjs","sources":["../lib/model.js","../lib/validate.js","../lib/main.js"],"sourcesContent":["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable no-restricted-syntax, no-invalid-this */\n\n'use strict';\n\n// MODULES //\n\nimport setReadOnly from '@stdlib/utils-define-nonenumerable-read-only-property';\nimport setReadOnlyAccessor from '@stdlib/utils-define-nonenumerable-read-only-accessor';\nimport format from '@stdlib/error-tools-fmtprodmsg';\nimport { ndarray as gdot } from '@stdlib/blas-base-gdot';\nimport { ndarray as gaxpy } from '@stdlib/blas-base-gaxpy';\nimport dcopy from '@stdlib/blas-base-dcopy';\nimport dscal from '@stdlib/blas-base-dscal';\nimport max from '@stdlib/math-base-special-max';\nimport exp from '@stdlib/math-base-special-exp';\nimport pow from '@stdlib/math-base-special-pow';\nimport sigmoid from '@stdlib/math-base-special-expit';\nimport Float64Array from '@stdlib/array-float64';\nimport ndarray from '@stdlib/ndarray-ctor';\nimport shape2strides from '@stdlib/ndarray-base-shape2strides';\nimport numel from '@stdlib/ndarray-base-numel';\nimport vind2bind from '@stdlib/ndarray-base-vind2bind';\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1.0e-7;\nvar MIN_SCALE = 1.0e-11;\nvar LEARNING_RATE_METHODS = {\n\t'basic': '_basicLearningRate',\n\t'constant': '_constantLearningRate',\n\t'invscaling': '_inverseScalingLearningRate',\n\t'pegasos': '_pegasosLearningRate'\n};\nvar LOSS_METHODS = {\n\t'hinge': '_hingeLoss',\n\t'log': '_logLoss',\n\t'modifiedHuber': '_modifiedHuberLoss',\n\t'perceptron': '_perceptronLoss',\n\t'squaredHinge': '_squaredHingeLoss'\n};\n\n\n// MAIN //\n\n/**\n* Model constructor.\n*\n* ## Notes\n*\n* -   The model (weight vector) implementation is inspired by the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @constructor\n* @param {PositiveInteger} N - number of feature weights (excluding bias/intercept term)\n* @param {Options} opts - model options\n* @param {PositiveNumber} opts.lambda - regularization parameter\n* @param {ArrayLikeObject} opts.learningRate - learning rate function and associated parameters\n* @param {string} opts.loss - loss function\n* @param {boolean} opts.intercept - boolean indicating whether to include an intercept\n* @returns {Model} model\n*/\nfunction Model( N, opts ) {\n\tvar len;\n\n\t// Set internal properties:\n\tthis._N = N;\n\tthis._opts = opts;\n\n\tthis._scaleFactor = 1.0;\n\tthis._t = 0; // iteration counter (i.e., number of updates)\n\n\t// Determine the learning rate function:\n\tthis._learningRateMethod = LEARNING_RATE_METHODS[ opts.learningRate[ 0 ] ];\n\n\t// Determine the loss function:\n\tthis._lossMethod = LOSS_METHODS[ opts.loss ];\n\n\t// Determine the number of model coefficients:\n\tlen = N;\n\tif ( opts.intercept ) {\n\t\tlen += 1;\n\t}\n\t// Initialize a model weight vector with all weights set to zero:\n\tthis._weights = new Float64Array( len );\n\n\t// Initialize model coefficients to zero:\n\tthis._coefficients = new ndarray( 'float64', new Float64Array( len ), [ len ], [ 1 ], 0, 'row-major' );\n\n\treturn this;\n}\n\n/**\n* Adds a provided input vector to the model weight vector.\n*\n* @private\n* @name _add\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - input vector\n* @param {number} scale - scale factor\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_add', function add( x, scale ) {\n\tvar s = scale / this._scaleFactor;\n\tvar w = this._weights;\n\n\t// Scale `x` and add to the model weight vector:\n\tgaxpy( x.shape[ 0 ], s, x.data, x.strides[ 0 ], x.offset, w, 1, 0 );\n\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this._opts.intercept ) {\n\t\tw[ this._N ] += s;\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate.\n*\n* ## Notes\n*\n* -   This learning rate function is based on the learning rate function of the same name in the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @name _basicLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_basicLearningRate', function basic() {\n\treturn 10.0 / ( 10.0+this._t );\n});\n\n/**\n* Returns a constant learning rate.\n*\n* @private\n* @name _constantLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_constantLearningRate', function constant() {\n\treturn this._opts.learningRate[ 1 ];\n});\n\n/**\n* Calculates the dot product of the model weight vector and a provided vector `x`.\n*\n* @private\n* @name _dot\n* @memberof Model.prototype\n* @type {Function}\n* @param {NumericArray} buf - ndarray data buffer\n* @param {integer} stride - stride\n* @param {NonNegativeInteger} offset - index offset\n* @returns {number} dot product\n*/\nsetReadOnly( Model.prototype, '_dot', function dot( buf, stride, offset ) {\n\tvar v = gdot( this._N, this._weights, 1, 0, buf, stride, offset );\n\tif ( this._opts.intercept ) {\n\t\tv += this._weights[ this._N ];\n\t}\n\tv *= this._scaleFactor;\n\treturn v;\n});\n\n/**\n* Updates the model weight vector using the hinge loss function.\n*\n* ## Notes\n*\n* -   The hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _hingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_hingeLoss', function hingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) < 1.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate according to an inverse scaling formula.\n*\n* ## Notes\n*\n* -   The inverse scaling formula is defined as\n*\n*     ```tex\n*     \\eta = \\frac{\\eta_0}{t^{k}}\n*     ```\n*\n*     where \\\\(\\eta_0\\\\) is an initial learning rate, \\\\(t\\\\) is the current iteration, and \\\\(k\\\\) is an exponent controlling how quickly the learning rate decreases.\n*\n* @private\n* @name _inverseScalingLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_inverseScalingLearningRate', function invscaling() {\n\tvar params = this._opts.learningRate;\n\treturn params[ 1 ] / pow( this._t, params[ 2 ] );\n});\n\n/**\n* Updates the model weight vector using the log loss function.\n*\n* ## Notes\n*\n* -   The log loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\ln( 1 + \\exp( -y\\,f(x) ) )\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _logLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_logLoss', function logLoss( x, y ) {\n\tvar loss;\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tloss = y / ( 1.0 + exp( y*d ) );\n\tthis._add( x, eta*loss );\n\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the modified Huber loss function.\n*\n* ## Notes\n*\n* -   The modified Huber loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\begin{cases}\n*       \\max(0, 1 - y\\,f(x))^2 & \\textrm{for}\\,\\,y\\,f(x) \\geq -1\\\\\n*       -4y\\,f(x) & \\textrm{otherwise}\n*     \\end{cases}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* ## References\n*\n* -   Zhang, Tong. 2004. \"Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms.\" In _Proceedings of the Twenty-First International Conference on Machine Learning_, 116. New York, NY, USA: Association for Computing Machinery. doi:[10.1145/1015330.1015332][@zhang:2004a].\n*\n* [@zhang:2004a]: https://doi.org/10.1145/1015330.1015332\n*\n* @private\n* @name _modifiedHuberLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_modifiedHuberLoss', function modifiedHuber( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < -1.0 ) {\n\t\tthis._add( x, 4.0*eta*y );\n\t} else {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate using Pegasos.\n*\n* ## References\n*\n* -   Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. \"Pegasos: primal estimated sub-gradient solver for SVM.\" _Mathematical Programming_ 127 (1): 3–30. doi:[10.1007/s10107-010-0420-4][@shalevshwartz:2011a].\n*\n* [@shalevshwartz:2011a]: https://doi.org/10.1007/s10107-010-0420-4\n*\n* @private\n* @name _pegasos\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_pegasosLearningRate', function pegasos() {\n\treturn 1.0 / ( this._opts.lambda*this._t );\n});\n\n/**\n* Updates the model weight vector using the perceptron loss function.\n*\n* ## Notes\n*\n* -   The perceptron loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max(0, -y\\,f(x))\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* -   The perceptron loss function is equivalent to the hinge loss function without a margin.\n*\n* -   The perceptron loss function does not update the model weight vector when the response is correctly classified.\n*\n* ## References\n*\n* -   Rosenblatt, Frank. 1957. \"The Perceptron–a perceiving and recognizing automaton.\" 85-460-1. Buffalo, NY, USA: Cornell Aeronautical Laboratory.\n*\n* @private\n* @name _perceptronLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_perceptronLoss', function perceptron( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) <= 0.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Performs L2 regularization of the model weights.\n*\n* @private\n* @name _regularize\n* @memberof Model.prototype\n* @type {Function}\n* @param {PositiveNumber} eta - learning rate\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_regularize', function regularize( eta ) {\n\tvar lambda = this._opts.lambda;\n\tif ( lambda <= 0.0 ) {\n\t\treturn this;\n\t}\n\tthis._scale( max( 1.0-( eta*lambda ), MIN_SCALING_FACTOR ) );\n\treturn this;\n});\n\n/**\n* Scale the model weight vector by a provided scaling factor.\n*\n* @private\n* @name _scale\n* @memberof Model.prototype\n* @type {Function}\n* @param {number} factor - scaling factor\n* @throws {RangeError} scaling factor must be a positive number\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_scale', function scale( factor ) {\n\tvar s;\n\tif ( factor <= 0.0 ) {\n\t\tthrow new RangeError( format( '0h1Eb', factor ) );\n\t}\n\t// Check whether we need to scale the weight vector to unity in order to avoid numerical issues...\n\ts = this._scaleFactor;\n\tif ( s < MIN_SCALE ) {\n\t\t// Note: we only scale/shrink the feature weights, not the intercept...\n\t\tdscal( this._N, s, this._weights, 1 );\n\t\tthis._scaleFactor = 1.0;\n\t}\n\tthis._scaleFactor *= factor;\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the squared hinge loss function.\n*\n* ## Notes\n*\n* -   The squared hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}^2\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _squaredHingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_squaredHingeLoss', function squaredHingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < 1.0 ) {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Returns the model coefficients.\n*\n* @private\n* @name coefficients\n* @memberof Model.prototype\n* @type {Function}\n* @returns {ndarray} model coefficients\n*/\nsetReadOnlyAccessor( Model.prototype, 'coefficients', function coefficients() {\n\tvar c = this._coefficients.data;\n\tvar w = this._weights;\n\tdcopy( w.length, w, 1, c, 1 );\n\tdscal( this._N, this._scaleFactor, c, 1 );\n\treturn this._coefficients;\n});\n\n/**\n* Returns the number of model features.\n*\n* @private\n* @name nfeatures\n* @memberof Model.prototype\n* @type {PositiveInteger}\n*/\nsetReadOnlyAccessor( Model.prototype, 'nfeatures', function nfeatures() {\n\treturn this._N;\n});\n\n/**\n* Predicts the response value for one or more observation vectors `X`.\n*\n* @private\n* @name predict\n* @memberof Model.prototype\n* @type {Function}\n* @param {ndarray} X - feature vector\n* @param {string} type - prediction type\n* @returns {ndarray} ndarray containing response values\n*/\nsetReadOnly( Model.prototype, 'predict', function predict( X, type ) {\n\tvar ndims;\n\tvar xbuf;\n\tvar ybuf;\n\tvar xsh;\n\tvar ysh;\n\tvar ord;\n\tvar ptr;\n\tvar sxn;\n\tvar sx;\n\tvar sy;\n\tvar ox;\n\tvar M;\n\tvar N;\n\tvar Y;\n\tvar v;\n\tvar i;\n\n\t// Cache input array properties in case of lazy evaluation:\n\txbuf = X.data;\n\txsh = X.shape;\n\tsx = X.strides;\n\tox = X.offset;\n\tord = X.order;\n\n\tndims = xsh.length - 1;\n\n\t// The output array shape is the same as the input array shape without the last dimension (i.e., the number of dimensions is reduced by one)...\n\tysh = [];\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tysh.push( xsh[ i ] );\n\t}\n\t// Create an output array...\n\tif ( ndims === 0 ) {\n\t\tM = 1;\n\t\tybuf = new Float64Array( 1 );\n\t\tsy = [ 0 ];\n\t} else {\n\t\tM = numel( ysh );\n\t\tybuf = new Float64Array( M );\n\t\tsy = shape2strides( ysh, ord );\n\t}\n\tY = new ndarray( 'int8', ybuf, ysh, sy, 0, ord );\n\n\t// Loop over all observation vectors...\n\tN = this._N; // number of features (i.e., size of last `X` dimension)\n\tsxn = sx[ ndims ]; // stride of the last `X` dimension\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Compute the index offset into the underlying data buffer pointing to the start of the current observation vector:\n\t\tptr = vind2bind( xsh, sx, ox, ord, i*N, 'throw' );\n\n\t\t// Compute the dot product of the current observation vector with the model weight vector:\n\t\tv = this._dot( xbuf, sxn, ptr );\n\n\t\t// Determine the output value:\n\t\tif ( type === 'label' ) {\n\t\t\tv = ( v > 0 ) ? 1 : -1;\n\t\t} else if ( type === 'probability' ) {\n\t\t\tv = sigmoid( v );\n\t\t} // else type === 'linear' (i.e., linear predictor)\n\n\t\t// Set the element in the output array:\n\t\tif ( ndims === 0 ) {\n\t\t\tY.iset( v );\n\t\t} else {\n\t\t\tY.iset( i, v );\n\t\t}\n\t}\n\treturn Y;\n});\n\n/**\n* Updates a model given a provided observation vector and response value.\n*\n* @private\n* @name update\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, 'update', function update( x, y ) {\n\tthis._t += 1;\n\treturn this[ this._lossMethod ]( x, y );\n});\n\n\n// EXPORTS //\n\nexport default Model;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isNonNegativeNumber } from '@stdlib/assert-is-nonnegative-number';\nimport { isPrimitive as isPositiveNumber } from '@stdlib/assert-is-positive-number';\nimport { isPrimitive as isNumber } from '@stdlib/assert-is-number';\nimport { isPrimitive as isBoolean } from '@stdlib/assert-is-boolean';\nimport isArrayLikeObject from '@stdlib/assert-is-array-like-object';\nimport isObject from '@stdlib/assert-is-plain-object';\nimport hasOwnProp from '@stdlib/assert-has-own-property';\nimport { factory as contains } from '@stdlib/array-base-assert-contains';\nimport format from '@stdlib/error-tools-fmtprodmsg';\nimport LEARNING_RATES from './learning_rates.json';\nimport LOSS_FUNCTIONS from './loss_functions.json';\n\n\n// VARIABLES //\n\nvar isLearningRate = contains( LEARNING_RATES );\nvar isLossFunction = contains( LOSS_FUNCTIONS );\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate] - learning rate function\n* @param {string} [options.loss] - loss function\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tvar name;\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( '0h12V', options ) );\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( '0h12o', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( '0h14k', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\tif ( !isArrayLikeObject( options.learningRate ) ) {\n\t\t\treturn new TypeError( format( '0h14l', 'learningRate', options.learningRate ) );\n\t\t}\n\t\tname = options.learningRate[ 0 ];\n\t\topts.learningRate[ 0 ] = name;\n\t\tif ( !isLearningRate( name ) ) {\n\t\t\treturn new TypeError( format( '0h14m', 'learningRate', LEARNING_RATES.join( '\", \"' ), name ) );\n\t\t}\n\t\tif ( options.learningRate.length > 1 ) {\n\t\t\tif ( name === 'constant' || name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 1 ] = options.learningRate[ 1 ];\n\t\t\t\tif ( !isPositiveNumber( opts.learningRate[ 1 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( '0h14n', 'learningRate', opts.learningRate[ 1 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ( options.learningRate.length > 2 ) {\n\t\t\tif ( name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 2 ] = options.learningRate[ 2 ];\n\t\t\t\tif ( !isNumber( opts.learningRate[ 2 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( '0h14o', 'learningRate', opts.learningRate[ 2 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !isLossFunction( opts.loss ) ) {\n\t\t\treturn new TypeError( format( '0h14S', 'loss', LOSS_FUNCTIONS.join( '\", \"' ), opts.loss ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nexport default validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isPositiveInteger } from '@stdlib/assert-is-positive-integer';\nimport isVectorLike from '@stdlib/assert-is-vector-like';\nimport isndarrayLike from '@stdlib/assert-is-ndarray-like';\nimport setReadOnly from '@stdlib/utils-define-nonenumerable-read-only-property';\nimport format from '@stdlib/error-tools-fmtprodmsg';\nimport Model from './model.js';\nimport LEARNING_RATE_DEFAULTS from './learning_rate_defaults.json';\nimport validate from './validate.js';\n\n\n// MAIN //\n\n/**\n* Returns an accumulator function which incrementally performs binary classification using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* -   The sub-gradient of the loss function is estimated for each datum and the classification model is updated incrementally, with a decreasing learning rate and regularization of model feature weights using L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3–30. doi:10.1007/s10107-010-0420-4\n*\n* @param {PositiveInteger} N - number of features\n* @param {Options} [options] - options object\n* @param {PositiveNumber} [options.lambda=1.0e-3] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate=['basic']] - learning rate function and associated parameters (one of `basic`, `constant`, or `pegasos`)\n* @param {string} [options.loss='log'] - loss function (one of `hinge`, `log`, `modifiedHuber`, `perceptron`, or `squaredHinge`)\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Function} accumulator\n*\n* @example\n* import Float64Array from '@stdlib/array-float64';\n* import array from '@stdlib/ndarray-array';\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\nfunction incrBinaryClassification( N, options ) {\n\tvar model;\n\tvar opts;\n\tvar err;\n\n\tif ( !isPositiveInteger( N ) ) {\n\t\tthrow new TypeError( format( '0h14b', N ) );\n\t}\n\topts = {\n\t\t'intercept': true,\n\t\t'lambda': 1.0e-4,\n\t\t'learningRate': LEARNING_RATE_DEFAULTS[ 'basic' ].slice(),\n\t\t'loss': 'log'\n\t};\n\tif ( arguments.length > 1 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\tmodel = new Model( N, opts );\n\n\t// Attach methods to the accumulator:\n\tsetReadOnly( accumulator, 'predict', predict );\n\n\treturn accumulator;\n\n\t/**\n\t* If provided a feature vector and response value, the accumulator function updates a binary classification model; otherwise, the accumulator function returns the current binary classification model coefficients.\n\t*\n\t* @private\n\t* @param {VectorLike} x - feature vector\n\t* @param {integer} y - response value\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray whose length matches the number of model features\n\t* @throws {TypeError} second argument must be either `+1` or `-1`\n\t* @returns {ndarray} one-dimensional ndarray containing model coefficients\n\t*\n\t* @example\n\t* import Float64Array from '@stdlib/array-float64';\n\t* import array from '@stdlib/ndarray-array';\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Update the model:\n\t* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n\t* var coefs = accumulator( x, 1 );\n\t* // returns <ndarray>\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tif ( arguments.length === 0 ) {\n\t\t\treturn model.coefficients;\n\t\t}\n\t\tif ( !isVectorLike( x ) ) {\n\t\t\tthrow new TypeError( format( '0h14c', x ) );\n\t\t}\n\t\tif ( y !== -1 && y !== 1 ) {\n\t\t\tthrow new TypeError( format( '0h14d', y ) );\n\t\t}\n\t\tif ( x.shape[ 0 ] !== model.nfeatures ) {\n\t\t\tthrow new TypeError( format( '0h14e', model.nfeatures, x.shape[ 0 ] ) );\n\t\t}\n\t\tmodel.update( x, y );\n\t\treturn model.coefficients;\n\t}\n\n\t/**\n\t* Predicts the response value for one or more observation vectors `X`.\n\t*\n\t* @private\n\t* @param {ndarrayLike} X - ndarray (of size `(...,N)`) containing observation vectors\n\t* @param {string} [type=\"label\"] - prediction type (either `label`, `probability`, or `linear`)\n\t* @throws {TypeError} first argument must be an ndarray\n\t* @throws {TypeError} first argument must be an ndarray whose last dimension matches the number of model features\n\t* @throws {TypeError} second argument must be a recognized/supported prediction \"type\"\n\t* @throws {Error} second argument must be compatible with the model loss function\n\t* @returns {ndarray} ndarray (of size `(...)`) containing response values\n\t*\n\t* @example\n\t* import Float64Array from '@stdlib/array-float64';\n\t* import array from '@stdlib/ndarray-array';\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Create a new observation vector:\n\t* var x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n\t*\n\t* // Predict the response value:\n\t* var yhat = accumulator.predict( x );\n\t* // returns <ndarray>\n\t*/\n\tfunction predict( X, type ) {\n\t\tvar sh;\n\t\tvar t;\n\t\tif ( !isndarrayLike( X ) ) {\n\t\t\tthrow new TypeError( format( '0h14f', X ) );\n\t\t}\n\t\tsh = X.shape;\n\t\tif ( sh[ sh.length-1 ] !== N ) {\n\t\t\tthrow new TypeError( format( '0h1Ea', N, sh[ sh.length-1 ] ) );\n\t\t}\n\t\tt = 'label';\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( type === 'probability' ) {\n\t\t\t\tif ( opts.loss !== 'log' && opts.loss !== 'modifiedHuber' ) {\n\t\t\t\t\tthrow new Error( format( '0h14h', [ 'log', 'modifiedHuber' ].join( '\", \"' ), opts.loss ) );\n\t\t\t\t}\n\t\t\t} else if ( type !== 'label' && type !== 'linear' ) {\n\t\t\t\tthrow new TypeError( format( '0h14i', type ) );\n\t\t\t}\n\t\t\tt = type;\n\t\t}\n\t\treturn model.predict( X, t );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrBinaryClassification;\n"],"names":["LEARNING_RATE_METHODS","basic","constant","invscaling","pegasos","LOSS_METHODS","hinge","log","modifiedHuber","perceptron","squaredHinge","Model","N","opts","len","this","_N","_opts","_scaleFactor","_t","_learningRateMethod","learningRate","_lossMethod","loss","intercept","_weights","Float64Array","_coefficients","ndarray","setReadOnly","prototype","x","scale","s","w","gaxpy","shape","data","strides","offset","buf","stride","v","gdot","y","eta","_regularize","_dot","_add","params","pow","d","exp","lambda","_scale","max","factor","RangeError","format","dscal","setReadOnlyAccessor","c","dcopy","length","X","type","ndims","xbuf","ybuf","xsh","ysh","ord","ptr","sxn","sx","sy","ox","M","Y","i","order","push","numel","shape2strides","vind2bind","sigmoid","iset","isLearningRate","contains","LEARNING_RATES","isLossFunction","LOSS_FUNCTIONS","incrBinaryClassification","options","model","err","isPositiveInteger","TypeError","LEARNING_RATE_DEFAULTS","slice","arguments","name","isObject","hasOwnProp","isBoolean","isNonNegativeNumber","isArrayLikeObject","join","isPositiveNumber","isNumber","validate","accumulator","sh","t","isndarrayLike","Error","predict","coefficients","isVectorLike","nfeatures","update"],"mappings":";;8qFA4CA,IAEIA,EAAwB,CAC3BC,MAAS,qBACTC,SAAY,wBACZC,WAAc,8BACdC,QAAW,wBAERC,EAAe,CAClBC,MAAS,aACTC,IAAO,WACPC,cAAiB,qBACjBC,WAAc,kBACdC,aAAgB,qBAyBjB,SAASC,EAAOC,EAAGC,GAClB,IAAIC,EA0BJ,OAvBAC,KAAKC,GAAKJ,EACVG,KAAKE,MAAQJ,EAEbE,KAAKG,aAAe,EACpBH,KAAKI,GAAK,EAGVJ,KAAKK,oBAAsBpB,EAAuBa,EAAKQ,aAAc,IAGrEN,KAAKO,YAAcjB,EAAcQ,EAAKU,MAGtCT,EAAMF,EACDC,EAAKW,YACTV,GAAO,GAGRC,KAAKU,SAAW,IAAIC,EAAcZ,GAGlCC,KAAKY,cAAgB,IAAIC,EAAS,UAAW,IAAIF,EAAcZ,GAAO,CAAEA,GAAO,CAAE,GAAK,EAAG,aAElFC,IACR,CAaAc,EAAalB,EAAMmB,UAAW,QAAQ,SAAcC,EAAGC,GACtD,IAAIC,EAAID,EAAQjB,KAAKG,aACjBgB,EAAInB,KAAKU,SASb,OANAU,EAAOJ,EAAEK,MAAO,GAAKH,EAAGF,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,OAAQL,EAAG,EAAG,GAG3DnB,KAAKE,MAAMO,YACfU,EAAGnB,KAAKC,KAAQiB,GAEVlB,IACR,IAiBAc,EAAalB,EAAMmB,UAAW,sBAAsB,WACnD,OAAO,IAAS,GAAKf,KAAKI,GAC3B,IAWAU,EAAalB,EAAMmB,UAAW,yBAAyB,WACtD,OAAOf,KAAKE,MAAMI,aAAc,EACjC,IAcAQ,EAAalB,EAAMmB,UAAW,QAAQ,SAAcU,EAAKC,EAAQF,GAChE,IAAIG,EAAIC,EAAM5B,KAAKC,GAAID,KAAKU,SAAU,EAAG,EAAGe,EAAKC,EAAQF,GAKzD,OAJKxB,KAAKE,MAAMO,YACfkB,GAAK3B,KAAKU,SAAUV,KAAKC,KAE1B0B,GAAK3B,KAAKG,YAEX,IA6BAW,EAAalB,EAAMmB,UAAW,cAAc,SAAoBC,EAAGa,GAClE,IAAIC,EAUJ,OAPAA,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAGXD,EADH7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,QAC1B,GACdxB,KAAKiC,KAAMjB,EAAGa,EAAEC,GAEV9B,IACR,IAqBAc,EAAalB,EAAMmB,UAAW,+BAA+B,WAC5D,IAAImB,EAASlC,KAAKE,MAAMI,aACxB,OAAO4B,EAAQ,GAAMC,EAAKnC,KAAKI,GAAI8B,EAAQ,GAC5C,IA6BApB,EAAalB,EAAMmB,UAAW,YAAY,SAAkBC,EAAGa,GAC9D,IAAIrB,EACAsB,EACAM,EASJ,OAPAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAElBM,EAAIpC,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,QACzChB,EAAOqB,GAAM,EAAMQ,EAAKR,EAAEO,IAC1BpC,KAAKiC,KAAMjB,EAAGc,EAAItB,GAEXR,IACR,IAsCAc,EAAalB,EAAMmB,UAAW,sBAAsB,SAAwBC,EAAGa,GAC9E,IAAIC,EACAM,EAWJ,OATAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,IAElBM,EAAIP,EAAI7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,UACnC,EACTxB,KAAKiC,KAAMjB,EAAG,EAAIc,EAAID,GAEtB7B,KAAKiC,KAAMjB,EAAGc,GAAMD,EAAGO,EAAEP,IAEnB7B,IACR,IAiBAc,EAAalB,EAAMmB,UAAW,wBAAwB,WACrD,OAAO,GAAQf,KAAKE,MAAMoC,OAAOtC,KAAKI,GACvC,IAqCAU,EAAalB,EAAMmB,UAAW,mBAAmB,SAAqBC,EAAGa,GACxE,IAAIC,EAUJ,OAPAA,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAGXD,EADH7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,SACzB,GACfxB,KAAKiC,KAAMjB,EAAGa,EAAEC,GAEV9B,IACR,IAYAc,EAAalB,EAAMmB,UAAW,eAAe,SAAqBe,GACjE,IAAIQ,EAAStC,KAAKE,MAAMoC,OACxB,OAAKA,GAAU,GAGftC,KAAKuC,OAAQC,EAAK,EAAMV,EAAIQ,EApYJ,OAkYhBtC,IAIT,IAaAc,EAAalB,EAAMmB,UAAW,UAAU,SAAgB0B,GACvD,IAAIvB,EACJ,GAAKuB,GAAU,EACd,MAAM,IAAIC,WAAYC,EAAQ,QAASF,IAUxC,OAPAvB,EAAIlB,KAAKG,cAxZM,QA2ZdyC,EAAO5C,KAAKC,GAAIiB,EAAGlB,KAAKU,SAAU,GAClCV,KAAKG,aAAe,GAErBH,KAAKG,cAAgBsC,EACdzC,IACR,IA6BAc,EAAalB,EAAMmB,UAAW,qBAAqB,SAA2BC,EAAGa,GAChF,IAAIC,EACAM,EASJ,OAPAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,IAElBM,EAAIP,EAAI7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,SACpC,GACRxB,KAAKiC,KAAMjB,EAAGc,GAAMD,EAAGO,EAAEP,IAEnB7B,IACR,IAWA6C,EAAqBjD,EAAMmB,UAAW,gBAAgB,WACrD,IAAI+B,EAAI9C,KAAKY,cAAcU,KACvBH,EAAInB,KAAKU,SAGb,OAFAqC,EAAO5B,EAAE6B,OAAQ7B,EAAG,EAAG2B,EAAG,GAC1BF,EAAO5C,KAAKC,GAAID,KAAKG,aAAc2C,EAAG,GAC/B9C,KAAKY,aACb,IAUAiC,EAAqBjD,EAAMmB,UAAW,aAAa,WAClD,OAAOf,KAAKC,EACb,IAaAa,EAAalB,EAAMmB,UAAW,WAAW,SAAkBkC,EAAGC,GAC7D,IAAIC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAjE,EACAkE,EACApC,EACAqC,EAaJ,IAVAZ,EAAOH,EAAE3B,KACTgC,EAAML,EAAE5B,MACRsC,EAAKV,EAAE1B,QACPsC,EAAKZ,EAAEzB,OACPgC,EAAMP,EAAEgB,MAERd,EAAQG,EAAIN,OAAS,EAGrBO,EAAM,GACAS,EAAI,EAAGA,EAAIb,EAAOa,IACvBT,EAAIW,KAAMZ,EAAKU,IAiBhB,IAde,IAAVb,GACJW,EAAI,EACJT,EAAO,IAAI1C,EAAc,GACzBiD,EAAK,CAAE,KAEPE,EAAIK,EAAOZ,GACXF,EAAO,IAAI1C,EAAcmD,GACzBF,EAAKQ,EAAeb,EAAKC,IAE1BO,EAAI,IAAIlD,EAAS,OAAQwC,EAAME,EAAKK,EAAI,EAAGJ,GAG3C3D,EAAIG,KAAKC,GACTyD,EAAMC,EAAIR,GACJa,EAAI,EAAGA,EAAIF,EAAGE,IAEnBP,EAAMY,EAAWf,EAAKK,EAAIE,EAAIL,EAAKQ,EAAEnE,EAAG,SAGxC8B,EAAI3B,KAAKgC,KAAMoB,EAAMM,EAAKD,GAGZ,UAATP,EACJvB,EAAMA,EAAI,EAAM,GAAK,EACD,gBAATuB,IACXvB,EAAI2C,EAAS3C,IAIC,IAAVwB,EACJY,EAAEQ,KAAM5C,GAERoC,EAAEQ,KAAMP,EAAGrC,GAGb,OAAOoC,CACR,IAaAjD,EAAalB,EAAMmB,UAAW,UAAU,SAAiBC,EAAGa,GAE3D,OADA7B,KAAKI,IAAM,EACJJ,KAAMA,KAAKO,aAAeS,EAAGa,EACrC,uNChlBI2C,EAAiBC,EAAUC,GAC3BC,EAAiBF,EAAUG,GC4C/B,SAASC,EAA0BhF,EAAGiF,GACrC,IAAIC,EACAjF,EACAkF,EAEJ,IAAMC,EAAmBpF,GACxB,MAAM,IAAIqF,UAAWvC,EAAQ,QAAS9C,IAQvC,GANAC,EAAO,CACNW,WAAa,EACb6B,OAAU,KACVhC,aAAgB6E,EAAiC,MAACC,QAClD5E,KAAQ,OAEJ6E,UAAUrC,OAAS,IACvBgC,EDlCF,SAAmBlF,EAAMgF,GACxB,IAAIQ,EACJ,IAAMC,EAAUT,GACf,OAAO,IAAII,UAAWvC,EAAQ,QAASmC,IAExC,GAAKU,EAAYV,EAAS,eACzBhF,EAAKW,UAAYqE,EAAQrE,WACnBgF,EAAW3F,EAAKW,YACrB,OAAO,IAAIyE,UAAWvC,EAAQ,QAAS,YAAa7C,EAAKW,YAG3D,GAAK+E,EAAYV,EAAS,YACzBhF,EAAKwC,OAASwC,EAAQxC,QAChBoD,EAAqB5F,EAAKwC,SAC/B,OAAO,IAAI4C,UAAWvC,EAAQ,QAAS,SAAU7C,EAAKwC,SAGxD,GAAKkD,EAAYV,EAAS,gBAAmB,CAC5C,IAAMa,EAAmBb,EAAQxE,cAChC,OAAO,IAAI4E,UAAWvC,EAAQ,QAAS,eAAgBmC,EAAQxE,eAIhE,GAFAgF,EAAOR,EAAQxE,aAAc,GAC7BR,EAAKQ,aAAc,GAAMgF,GACnBd,EAAgBc,GACrB,OAAO,IAAIJ,UAAWvC,EAAQ,QAAS,eAAgB+B,EAAekB,KAAM,QAAUN,IAEvF,GAAKR,EAAQxE,aAAa0C,OAAS,IACpB,aAATsC,GAAgC,eAATA,KAC3BxF,EAAKQ,aAAc,GAAMwE,EAAQxE,aAAc,IACzCuF,EAAkB/F,EAAKQ,aAAc,KAC1C,OAAO,IAAI4E,UAAWvC,EAAQ,QAAS,eAAgB7C,EAAKQ,aAAc,KAI7E,GAAKwE,EAAQxE,aAAa0C,OAAS,GACpB,eAATsC,IACJxF,EAAKQ,aAAc,GAAMwE,EAAQxE,aAAc,IACzCwF,EAAUhG,EAAKQ,aAAc,KAClC,OAAO,IAAI4E,UAAWvC,EAAQ,QAAS,eAAgB7C,EAAKQ,aAAc,IAI7E,CACD,OAAKkF,EAAYV,EAAS,UACzBhF,EAAKU,KAAOsE,EAAQtE,MACdmE,EAAgB7E,EAAKU,OACnB,IAAI0E,UAAWvC,EAAQ,QAAS,OAAQiC,EAAegB,KAAM,QAAU9F,EAAKU,OAG9E,IACR,CChBQuF,CAAUjG,EAAMgF,GACjBE,GACJ,MAAMA,EAQR,OALAD,EAAQ,IAAInF,EAAOC,EAAGC,GAGtBgB,EAAakF,EAAa,WA0E1B,SAAkB/C,EAAGC,GACpB,IAAI+C,EACAC,EACJ,IAAMC,EAAelD,GACpB,MAAM,IAAIiC,UAAWvC,EAAQ,QAASM,IAGvC,IADAgD,EAAKhD,EAAE5B,OACE4E,EAAGjD,OAAO,KAAQnD,EAC1B,MAAM,IAAIqF,UAAWvC,EAAQ,QAAS9C,EAAGoG,EAAIA,EAAGjD,OAAO,KAGxD,GADAkD,EAAI,QACCb,UAAUrC,OAAS,EAAI,CAC3B,GAAc,gBAATE,GACJ,GAAmB,QAAdpD,EAAKU,MAAgC,kBAAdV,EAAKU,KAChC,MAAM,IAAI4F,MAAOzD,EAAQ,QAAS,CAAE,MAAO,iBAAkBiD,KAAM,QAAU9F,EAAKU,YAE7E,GAAc,UAAT0C,GAA6B,WAATA,EAC/B,MAAM,IAAIgC,UAAWvC,EAAQ,QAASO,IAEvCgD,EAAIhD,CACJ,CACD,OAAO6B,EAAMsB,QAASpD,EAAGiD,EACzB,IA9FMF,EA2BP,SAASA,EAAahF,EAAGa,GACxB,GAA0B,IAArBwD,UAAUrC,OACd,OAAO+B,EAAMuB,aAEd,IAAMC,EAAcvF,GACnB,MAAM,IAAIkE,UAAWvC,EAAQ,QAAS3B,IAEvC,IAAY,IAAPa,GAAkB,IAANA,EAChB,MAAM,IAAIqD,UAAWvC,EAAQ,QAASd,IAEvC,GAAKb,EAAEK,MAAO,KAAQ0D,EAAMyB,UAC3B,MAAM,IAAItB,UAAWvC,EAAQ,QAASoC,EAAMyB,UAAWxF,EAAEK,MAAO,KAGjE,OADA0D,EAAM0B,OAAQzF,EAAGa,GACVkD,EAAMuB,YACb,CAqDF"}