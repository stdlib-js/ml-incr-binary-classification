{"version":3,"file":"index.mjs","sources":["../lib/model.js","../lib/validate.js","../lib/main.js"],"sourcesContent":["/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n/* eslint-disable no-restricted-syntax, no-invalid-this */\n\n'use strict';\n\n// MODULES //\n\nimport setReadOnly from '@stdlib/utils-define-nonenumerable-read-only-property' ;\nimport setReadOnlyAccessor from '@stdlib/utils-define-nonenumerable-read-only-accessor' ;\nimport format from '@stdlib/error-tools-fmtprodmsg' ;\nimport { ndarray as gdot } from '@stdlib/blas-base-gdot' ;\nimport { ndarray as gaxpy } from '@stdlib/blas-base-gaxpy' ;\nimport dcopy from '@stdlib/blas-base-dcopy' ;\nimport dscal from '@stdlib/blas-base-dscal' ;\nimport max from '@stdlib/math-base-special-max' ;\nimport exp from '@stdlib/math-base-special-exp' ;\nimport pow from '@stdlib/math-base-special-pow' ;\nimport sigmoid from '@stdlib/math-base-special-expit' ;\nimport Float64Array from '@stdlib/array-float64' ;\nimport ndarray from '@stdlib/ndarray-ctor' ;\nimport shape2strides from '@stdlib/ndarray-base-shape2strides' ;\nimport numel from '@stdlib/ndarray-base-numel' ;\nimport vind2bind from '@stdlib/ndarray-base-vind2bind' ;\n\n\n// VARIABLES //\n\nvar MIN_SCALING_FACTOR = 1.0e-7;\nvar MIN_SCALE = 1.0e-11;\nvar LEARNING_RATE_METHODS = {\n\t'basic': '_basicLearningRate',\n\t'constant': '_constantLearningRate',\n\t'invscaling': '_inverseScalingLearningRate',\n\t'pegasos': '_pegasosLearningRate'\n};\nvar LOSS_METHODS = {\n\t'hinge': '_hingeLoss',\n\t'log': '_logLoss',\n\t'modifiedHuber': '_modifiedHuberLoss',\n\t'perceptron': '_perceptronLoss',\n\t'squaredHinge': '_squaredHingeLoss'\n};\n\n\n// MAIN //\n\n/**\n* Model constructor.\n*\n* ## Notes\n*\n* -   The model (weight vector) implementation is inspired by the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @constructor\n* @param {PositiveInteger} N - number of feature weights (excluding bias/intercept term)\n* @param {Options} opts - model options\n* @param {PositiveNumber} opts.lambda - regularization parameter\n* @param {ArrayLikeObject} opts.learningRate - learning rate function and associated parameters\n* @param {string} opts.loss - loss function\n* @param {boolean} opts.intercept - boolean indicating whether to include an intercept\n* @returns {Model} model\n*/\nfunction Model( N, opts ) {\n\tvar len;\n\n\t// Set internal properties:\n\tthis._N = N;\n\tthis._opts = opts;\n\n\tthis._scaleFactor = 1.0;\n\tthis._t = 0; // iteration counter (i.e., number of updates)\n\n\t// Determine the learning rate function:\n\tthis._learningRateMethod = LEARNING_RATE_METHODS[ opts.learningRate[ 0 ] ];\n\n\t// Determine the loss function:\n\tthis._lossMethod = LOSS_METHODS[ opts.loss ];\n\n\t// Determine the number of model coefficients:\n\tlen = N;\n\tif ( opts.intercept ) {\n\t\tlen += 1;\n\t}\n\t// Initialize a model weight vector with all weights set to zero:\n\tthis._weights = new Float64Array( len );\n\n\t// Initialize model coefficients to zero:\n\tthis._coefficients = new ndarray( 'float64', new Float64Array( len ), [ len ], [ 1 ], 0, 'row-major' );\n\n\treturn this;\n}\n\n/**\n* Adds a provided input vector to the model weight vector.\n*\n* @private\n* @name _add\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - input vector\n* @param {number} scale - scale factor\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_add', function add( x, scale ) {\n\tvar s = scale / this._scaleFactor;\n\tvar w = this._weights;\n\n\t// Scale `x` and add to the model weight vector:\n\tgaxpy( x.shape[ 0 ], s, x.data, x.strides[ 0 ], x.offset, w, 1, 0 );\n\n\t// If an intercept is assumed, treat `x` as containing one additional element equal to one...\n\tif ( this._opts.intercept ) {\n\t\tw[ this._N ] += s;\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate.\n*\n* ## Notes\n*\n* -   This learning rate function is based on the learning rate function of the same name in the [sofia-ml][sofia-ml] library.\n*\n* [sofia-ml]: https://code.google.com/archive/p/sofia-ml/\n*\n* @private\n* @name _basicLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_basicLearningRate', function basic() {\n\treturn 10.0 / ( 10.0+this._t );\n});\n\n/**\n* Returns a constant learning rate.\n*\n* @private\n* @name _constantLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_constantLearningRate', function constant() {\n\treturn this._opts.learningRate[ 1 ];\n});\n\n/**\n* Calculates the dot product of the model weight vector and a provided vector `x`.\n*\n* @private\n* @name _dot\n* @memberof Model.prototype\n* @type {Function}\n* @param {NumericArray} buf - ndarray data buffer\n* @param {integer} stride - stride\n* @param {NonNegativeInteger} offset - index offset\n* @returns {number} dot product\n*/\nsetReadOnly( Model.prototype, '_dot', function dot( buf, stride, offset ) {\n\tvar v = gdot( this._N, this._weights, 1, 0, buf, stride, offset );\n\tif ( this._opts.intercept ) {\n\t\tv += this._weights[ this._N ];\n\t}\n\tv *= this._scaleFactor;\n\treturn v;\n});\n\n/**\n* Updates the model weight vector using the hinge loss function.\n*\n* ## Notes\n*\n* -   The hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _hingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_hingeLoss', function hingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) < 1.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate according to an inverse scaling formula.\n*\n* ## Notes\n*\n* -   The inverse scaling formula is defined as\n*\n*     ```tex\n*     \\eta = \\frac{\\eta_0}{t^{k}}\n*     ```\n*\n*     where \\\\(\\eta_0\\\\) is an initial learning rate, \\\\(t\\\\) is the current iteration, and \\\\(k\\\\) is an exponent controlling how quickly the learning rate decreases.\n*\n* @private\n* @name _inverseScalingLearningRate\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_inverseScalingLearningRate', function invscaling() {\n\tvar params = this._opts.learningRate;\n\treturn params[ 1 ] / pow( this._t, params[ 2 ] );\n});\n\n/**\n* Updates the model weight vector using the log loss function.\n*\n* ## Notes\n*\n* -   The log loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\ln( 1 + \\exp( -y\\,f(x) ) )\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n*\n* @private\n* @name _logLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_logLoss', function logLoss( x, y ) {\n\tvar loss;\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tloss = y / ( 1.0 + exp( y*d ) );\n\tthis._add( x, eta*loss );\n\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the modified Huber loss function.\n*\n* ## Notes\n*\n* -   The modified Huber loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\begin{cases}\n*       \\max(0, 1 - y\\,f(x))^2 & \\textrm{for}\\,\\,y\\,f(x) \\geq -1\\\\\n*       -4y\\,f(x) & \\textrm{otherwise}\n*     \\end{cases}\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* ## References\n*\n* -   Zhang, Tong. 2004. \"Solving Large Scale Linear Prediction Problems Using Stochastic Gradient Descent Algorithms.\" In _Proceedings of the Twenty-First International Conference on Machine Learning_, 116. New York, NY, USA: Association for Computing Machinery. doi:[10.1145/1015330.1015332][@zhang:2004a].\n*\n* [@zhang:2004a]: https://doi.org/10.1145/1015330.1015332\n*\n* @private\n* @name _modifiedHuberLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_modifiedHuberLoss', function modifiedHuber( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < -1.0 ) {\n\t\tthis._add( x, 4.0*eta*y );\n\t} else {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Computes a learning rate using Pegasos.\n*\n* ## References\n*\n* -   Shalev-Shwartz, Shai, Yoram Singer, Nathan Srebro, and Andrew Cotter. 2011. \"Pegasos: primal estimated sub-gradient solver for SVM.\" _Mathematical Programming_ 127 (1): 3–30. doi:[10.1007/s10107-010-0420-4][@shalevshwartz:2011a].\n*\n* [@shalevshwartz:2011a]: https://doi.org/10.1007/s10107-010-0420-4\n*\n* @private\n* @name _pegasos\n* @memberof Model.prototype\n* @type {Function}\n* @returns {number} learning rate\n*/\nsetReadOnly( Model.prototype, '_pegasosLearningRate', function pegasos() {\n\treturn 1.0 / ( this._opts.lambda*this._t );\n});\n\n/**\n* Updates the model weight vector using the perceptron loss function.\n*\n* ## Notes\n*\n* -   The perceptron loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max(0, -y\\,f(x))\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* -   The perceptron loss function is equivalent to the hinge loss function without a margin.\n*\n* -   The perceptron loss function does not update the model weight vector when the response is correctly classified.\n*\n* ## References\n*\n* -   Rosenblatt, Frank. 1957. \"The Perceptron–a perceiving and recognizing automaton.\" 85-460-1. Buffalo, NY, USA: Cornell Aeronautical Laboratory.\n*\n* @private\n* @name _perceptronLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_perceptronLoss', function perceptron( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( ( y*d ) <= 0.0 ) {\n\t\tthis._add( x, y*eta );\n\t}\n\treturn this;\n});\n\n/**\n* Performs L2 regularization of the model weights.\n*\n* @private\n* @name _regularize\n* @memberof Model.prototype\n* @type {Function}\n* @param {PositiveNumber} eta - learning rate\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_regularize', function regularize( eta ) {\n\tvar lambda = this._opts.lambda;\n\tif ( lambda <= 0.0 ) {\n\t\treturn this;\n\t}\n\tthis._scale( max( 1.0-( eta*lambda ), MIN_SCALING_FACTOR ) );\n\treturn this;\n});\n\n/**\n* Scale the model weight vector by a provided scaling factor.\n*\n* @private\n* @name _scale\n* @memberof Model.prototype\n* @type {Function}\n* @param {number} factor - scaling factor\n* @throws {RangeError} scaling factor must be a positive number\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_scale', function scale( factor ) {\n\tvar s;\n\tif ( factor <= 0.0 ) {\n\t\tthrow new RangeError( format( 'invalid argument. Attempting to scale a weight vector by a nonpositive value. This is likely due to too large a value of eta * lambda. Value: `%f`.', factor ) );\n\t}\n\t// Check whether we need to scale the weight vector to unity in order to avoid numerical issues...\n\ts = this._scaleFactor;\n\tif ( s < MIN_SCALE ) {\n\t\t// Note: we only scale/shrink the feature weights, not the intercept...\n\t\tdscal( this._N, s, this._weights, 1 );\n\t\tthis._scaleFactor = 1.0;\n\t}\n\tthis._scaleFactor *= factor;\n\treturn this;\n});\n\n/**\n* Updates the model weight vector using the squared hinge loss function.\n*\n* ## Notes\n*\n* -   The squared hinge loss function is defined as\n*\n*     ```tex\n*     L(y, f(x)) = \\max\\{ 0, 1 - y\\,f(x) \\}^2\n*     ```\n*\n*     where\n*\n*     ```tex\n*     f(x) = w^T x + b\n*     ```\n*\n*     with \\\\(w\\\\) being the model weight vector and \\\\(b\\\\) being the intercept.\n*\n* @private\n* @name _squaredHingeLoss\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, '_squaredHingeLoss', function squaredHingeLoss( x, y ) {\n\tvar eta;\n\tvar d;\n\n\teta = this[ this._learningRateMethod ]();\n\tthis._regularize( eta );\n\n\td = y * this._dot( x.data, x.strides[ 0 ], x.offset );\n\tif ( d < 1.0 ) {\n\t\tthis._add( x, eta*( y-(d*y) ) );\n\t}\n\treturn this;\n});\n\n/**\n* Returns the model coefficients.\n*\n* @private\n* @name coefficients\n* @memberof Model.prototype\n* @type {Function}\n* @returns {ndarray} model coefficients\n*/\nsetReadOnlyAccessor( Model.prototype, 'coefficients', function coefficients() {\n\tvar c = this._coefficients.data;\n\tvar w = this._weights;\n\tdcopy( w.length, w, 1, c, 1 );\n\tdscal( this._N, this._scaleFactor, c, 1 );\n\treturn this._coefficients;\n});\n\n/**\n* Returns the number of model features.\n*\n* @private\n* @name nfeatures\n* @memberof Model.prototype\n* @type {PositiveInteger}\n*/\nsetReadOnlyAccessor( Model.prototype, 'nfeatures', function nfeatures() {\n\treturn this._N;\n});\n\n/**\n* Predicts the response value for one or more observation vectors `X`.\n*\n* @private\n* @name predict\n* @memberof Model.prototype\n* @type {Function}\n* @param {ndarray} X - feature vector\n* @param {string} type - prediction type\n* @returns {ndarray} ndarray containing response values\n*/\nsetReadOnly( Model.prototype, 'predict', function predict( X, type ) {\n\tvar ndims;\n\tvar xbuf;\n\tvar ybuf;\n\tvar xsh;\n\tvar ysh;\n\tvar ord;\n\tvar ptr;\n\tvar sxn;\n\tvar sx;\n\tvar sy;\n\tvar ox;\n\tvar M;\n\tvar N;\n\tvar Y;\n\tvar v;\n\tvar i;\n\n\t// Cache input array properties in case of lazy evaluation:\n\txbuf = X.data;\n\txsh = X.shape;\n\tsx = X.strides;\n\tox = X.offset;\n\tord = X.order;\n\n\tndims = xsh.length - 1;\n\n\t// The output array shape is the same as the input array shape without the last dimension (i.e., the number of dimensions is reduced by one)...\n\tysh = [];\n\tfor ( i = 0; i < ndims; i++ ) {\n\t\tysh.push( xsh[ i ] );\n\t}\n\t// Create an output array...\n\tif ( ndims === 0 ) {\n\t\tM = 1;\n\t\tybuf = new Float64Array( 1 );\n\t\tsy = [ 0 ];\n\t} else {\n\t\tM = numel( ysh );\n\t\tybuf = new Float64Array( M );\n\t\tsy = shape2strides( ysh, ord );\n\t}\n\tY = new ndarray( 'int8', ybuf, ysh, sy, 0, ord );\n\n\t// Loop over all observation vectors...\n\tN = this._N; // number of features (i.e., size of last `X` dimension)\n\tsxn = sx[ ndims ]; // stride of the last `X` dimension\n\tfor ( i = 0; i < M; i++ ) {\n\t\t// Compute the index offset into the underlying data buffer pointing to the start of the current observation vector:\n\t\tptr = vind2bind( xsh, sx, ox, ord, i*N, 'throw' );\n\n\t\t// Compute the dot product of the current observation vector with the model weight vector:\n\t\tv = this._dot( xbuf, sxn, ptr );\n\n\t\t// Determine the output value:\n\t\tif ( type === 'label' ) {\n\t\t\tv = ( v > 0 ) ? 1 : -1;\n\t\t} else if ( type === 'probability' ) {\n\t\t\tv = sigmoid( v );\n\t\t} // else type === 'linear' (i.e., linear predictor)\n\n\t\t// Set the element in the output array:\n\t\tif ( ndims === 0 ) {\n\t\t\tY.iset( v );\n\t\t} else {\n\t\t\tY.iset( i, v );\n\t\t}\n\t}\n\treturn Y;\n});\n\n/**\n* Updates a model given a provided observation vector and response value.\n*\n* @private\n* @name update\n* @memberof Model.prototype\n* @type {Function}\n* @param {VectorLike} x - feature vector\n* @param {integer} y - response value\n* @returns {Model} model instance\n*/\nsetReadOnly( Model.prototype, 'update', function update( x, y ) {\n\tthis._t += 1;\n\treturn this[ this._lossMethod ]( x, y );\n});\n\n\n// EXPORTS //\n\nexport default Model;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isNonNegativeNumber } from '@stdlib/assert-is-nonnegative-number' ;\nimport { isPrimitive as isPositiveNumber } from '@stdlib/assert-is-positive-number' ;\nimport { isPrimitive as isNumber } from '@stdlib/assert-is-number' ;\nimport { isPrimitive as isBoolean } from '@stdlib/assert-is-boolean' ;\nimport isArrayLikeObject from '@stdlib/assert-is-array-like-object' ;\nimport isObject from '@stdlib/assert-is-plain-object' ;\nimport hasOwnProp from '@stdlib/assert-has-own-property' ;\nimport contains from '@stdlib/assert-contains' ;\nimport format from '@stdlib/error-tools-fmtprodmsg' ;\nimport LEARNING_RATES from './learning_rates.json' ;\nimport LOSS_FUNCTIONS from './loss_functions.json' ;\n\n\n// MAIN //\n\n/**\n* Validates function options.\n*\n* @private\n* @param {Object} opts - destination object\n* @param {Options} options - function options\n* @param {PositiveNumber} [options.lambda] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate] - learning rate function\n* @param {string} [options.loss] - loss function\n* @param {boolean} [options.intercept] - specifies whether an intercept should be included\n* @returns {(Error|null)} null or an error object\n*\n* @example\n* var opts = {};\n* var options = {};\n* var err = validate( opts, options );\n* if ( err ) {\n*     throw err;\n* }\n*/\nfunction validate( opts, options ) {\n\tvar name;\n\tif ( !isObject( options ) ) {\n\t\treturn new TypeError( format( '0LQ2h', options ) );\n\t}\n\tif ( hasOwnProp( options, 'intercept' ) ) {\n\t\topts.intercept = options.intercept;\n\t\tif ( !isBoolean( opts.intercept ) ) {\n\t\t\treturn new TypeError( format( '0LQ30', 'intercept', opts.intercept ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'lambda' ) ) {\n\t\topts.lambda = options.lambda;\n\t\tif ( !isNonNegativeNumber( opts.lambda ) ) {\n\t\t\treturn new TypeError( format( '0LQ4x', 'lambda', opts.lambda ) );\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'learningRate' ) ) {\n\t\tif ( !isArrayLikeObject( options.learningRate ) ) {\n\t\t\treturn new TypeError( format( '0LQ4y', 'learningRate', options.learningRate ) );\n\t\t}\n\t\tname = options.learningRate[ 0 ];\n\t\topts.learningRate[ 0 ] = name;\n\t\tif ( !contains( LEARNING_RATES, name ) ) {\n\t\t\treturn new TypeError( format( '0LQ4z', 'learningRate', LEARNING_RATES.join( '\", \"' ), name ) );\n\t\t}\n\t\tif ( options.learningRate.length > 1 ) {\n\t\t\tif ( name === 'constant' || name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 1 ] = options.learningRate[ 1 ];\n\t\t\t\tif ( !isPositiveNumber( opts.learningRate[ 1 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Second `%s` option must be a positive number. Option: `%s`.', 'learningRate', opts.learningRate[ 1 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif ( options.learningRate.length > 2 ) {\n\t\t\tif ( name === 'invscaling' ) {\n\t\t\t\topts.learningRate[ 2 ] = options.learningRate[ 2 ];\n\t\t\t\tif ( !isNumber( opts.learningRate[ 2 ] ) ) {\n\t\t\t\t\treturn new TypeError( format( 'invalid option. Third `%s` option must be a number. Option: `%s`.', 'learningRate', opts.learningRate[ 2 ] ) );\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\tif ( hasOwnProp( options, 'loss' ) ) {\n\t\topts.loss = options.loss;\n\t\tif ( !contains( LOSS_FUNCTIONS, opts.loss ) ) {\n\t\t\treturn new TypeError( format( '0LQ3t', 'loss', LOSS_FUNCTIONS.join( '\", \"' ), opts.loss ) );\n\t\t}\n\t}\n\treturn null;\n}\n\n\n// EXPORTS //\n\nexport default validate;\n","/**\n* @license Apache-2.0\n*\n* Copyright (c) 2018 The Stdlib Authors.\n*\n* Licensed under the Apache License, Version 2.0 (the \"License\");\n* you may not use this file except in compliance with the License.\n* You may obtain a copy of the License at\n*\n*    http://www.apache.org/licenses/LICENSE-2.0\n*\n* Unless required by applicable law or agreed to in writing, software\n* distributed under the License is distributed on an \"AS IS\" BASIS,\n* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n* See the License for the specific language governing permissions and\n* limitations under the License.\n*/\n\n'use strict';\n\n// MODULES //\n\nimport { isPrimitive as isPositiveInteger } from '@stdlib/assert-is-positive-integer' ;\nimport isVectorLike from '@stdlib/assert-is-vector-like' ;\nimport isndarrayLike from '@stdlib/assert-is-ndarray-like' ;\nimport setReadOnly from '@stdlib/utils-define-nonenumerable-read-only-property' ;\nimport format from '@stdlib/error-tools-fmtprodmsg' ;\nimport Model from './model.js' ;\nimport LEARNING_RATE_DEFAULTS from './learning_rate_defaults.json' ;\nimport validate from './validate.js' ;\n\n\n// MAIN //\n\n/**\n* Returns an accumulator function which incrementally performs binary classification using stochastic gradient descent (SGD).\n*\n* ## Method\n*\n* -   The sub-gradient of the loss function is estimated for each datum and the classification model is updated incrementally, with a decreasing learning rate and regularization of model feature weights using L2 regularization.\n*\n* ## References\n*\n* -   Shalev-Shwartz, S., Singer, Y., Srebro, N., & Cotter, A. (2011). Pegasos: Primal estimated sub-gradient solver for SVM. Mathematical Programming, 127(1), 3–30. doi:10.1007/s10107-010-0420-4\n*\n* @param {PositiveInteger} N - number of features\n* @param {Options} [options] - options object\n* @param {PositiveNumber} [options.lambda=1.0e-3] - regularization parameter\n* @param {ArrayLikeObject} [options.learningRate=['basic']] - learning rate function and associated parameters (one of `basic`, `constant`, or `pegasos`)\n* @param {string} [options.loss='log'] - loss function (one of `hinge`, `log`, `modifiedHuber`, `perceptron`, or `squaredHinge`)\n* @param {boolean} [options.intercept=true] - boolean indicating whether to include an intercept\n* @throws {TypeError} first argument must be a positive integer\n* @throws {TypeError} options argument must be an object\n* @throws {TypeError} must provide valid options\n* @returns {Function} accumulator\n*\n* @example\n* import Float64Array from '@stdlib/array-float64' ;\n* import array from '@stdlib/ndarray-array' ;\n*\n* // Create an accumulator:\n* var accumulator = incrBinaryClassification( 3, {\n*     'intercept': true,\n*     'lambda': 1.0e-5\n* });\n*\n* // ...\n*\n* // Update the model:\n* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n* var coefs = accumulator( x, 1 );\n* // returns <ndarray>\n*\n* // ...\n*\n* // Create a new observation vector:\n* x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n*\n* // Predict the response value:\n* var yhat = accumulator.predict( x );\n* // returns <ndarray>\n*/\nfunction incrBinaryClassification( N, options ) {\n\tvar model;\n\tvar opts;\n\tvar err;\n\n\tif ( !isPositiveInteger( N ) ) {\n\t\tthrow new TypeError( format( '0LQ4o', N ) );\n\t}\n\topts = {\n\t\t'intercept': true,\n\t\t'lambda': 1.0e-4,\n\t\t'learningRate': LEARNING_RATE_DEFAULTS[ 'basic' ].slice(),\n\t\t'loss': 'log'\n\t};\n\tif ( arguments.length > 1 ) {\n\t\terr = validate( opts, options );\n\t\tif ( err ) {\n\t\t\tthrow err;\n\t\t}\n\t}\n\tmodel = new Model( N, opts );\n\n\t// Attach methods to the accumulator:\n\tsetReadOnly( accumulator, 'predict', predict );\n\n\treturn accumulator;\n\n\t/**\n\t* If provided a feature vector and response value, the accumulator function updates a binary classification model; otherwise, the accumulator function returns the current binary classification model coefficients.\n\t*\n\t* @private\n\t* @param {VectorLike} x - feature vector\n\t* @param {integer} y - response value\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray\n\t* @throws {TypeError} first argument must be a one-dimensional ndarray whose length matches the number of model features\n\t* @throws {TypeError} second argument must be either `+1` or `-1`\n\t* @returns {ndarray} one-dimensional ndarray containing model coefficients\n\t*\n\t* @example\n\t* import Float64Array from '@stdlib/array-float64' ;\n\t* import array from '@stdlib/ndarray-array' ;\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Update the model:\n\t* var x = array( new Float64Array( [ 2.3, 1.0, 5.0 ] ) );\n\t* var coefs = accumulator( x, 1 );\n\t* // returns <ndarray>\n\t*/\n\tfunction accumulator( x, y ) {\n\t\tif ( arguments.length === 0 ) {\n\t\t\treturn model.coefficients;\n\t\t}\n\t\tif ( !isVectorLike( x ) ) {\n\t\t\tthrow new TypeError( format( '0LQ4p', x ) );\n\t\t}\n\t\tif ( y !== -1 && y !== 1 ) {\n\t\t\tthrow new TypeError( format( '0LQ4q', y ) );\n\t\t}\n\t\tif ( x.shape[ 0 ] !== model.nfeatures ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be a one-dimensional ndarray of length %u. Actual length: `%u`.', model.nfeatures, x.shape[ 0 ] ) );\n\t\t}\n\t\tmodel.update( x, y );\n\t\treturn model.coefficients;\n\t}\n\n\t/**\n\t* Predicts the response value for one or more observation vectors `X`.\n\t*\n\t* @private\n\t* @param {ndarrayLike} X - ndarray (of size `(...,N)`) containing observation vectors\n\t* @param {string} [type=\"label\"] - prediction type (either `label`, `probability`, or `linear`)\n\t* @throws {TypeError} first argument must be an ndarray\n\t* @throws {TypeError} first argument must be an ndarray whose last dimension matches the number of model features\n\t* @throws {TypeError} second argument must be a recognized/supported prediction \"type\"\n\t* @throws {Error} second argument must be compatible with the model loss function\n\t* @returns {ndarray} ndarray (of size `(...)`) containing response values\n\t*\n\t* @example\n\t* import Float64Array from '@stdlib/array-float64' ;\n\t* import array from '@stdlib/ndarray-array' ;\n\t*\n\t* // Create an accumulator:\n\t* var accumulator = incrBinaryClassification( 3 );\n\t*\n\t* // ...\n\t*\n\t* // Create a new observation vector:\n\t* var x = array( new Float64Array( [ 2.3, 5.3, 8.6 ] ) );\n\t*\n\t* // Predict the response value:\n\t* var yhat = accumulator.predict( x );\n\t* // returns <ndarray>\n\t*/\n\tfunction predict( X, type ) {\n\t\tvar sh;\n\t\tvar t;\n\t\tif ( !isndarrayLike( X ) ) {\n\t\t\tthrow new TypeError( format( '0LQ4s', X ) );\n\t\t}\n\t\tsh = X.shape;\n\t\tif ( sh[ sh.length-1 ] !== N ) {\n\t\t\tthrow new TypeError( format( 'invalid argument. First argument must be an ndarray whose last dimension is of size %u. Actual size: `%u`.', N, sh[ sh.length-1 ] ) );\n\t\t}\n\t\tt = 'label';\n\t\tif ( arguments.length > 1 ) {\n\t\t\tif ( type === 'probability' ) {\n\t\t\t\tif ( opts.loss !== 'log' && opts.loss !== 'modifiedHuber' ) {\n\t\t\t\t\tthrow new Error( format( 'invalid argument. Second argument is incompatible with model loss function. Probability predictions are only supported when the loss function is one of the following: \"%s\". Model loss function: `%s`.', [ 'log', 'modifiedHuber' ].join( '\", \"' ), opts.loss ) );\n\t\t\t\t}\n\t\t\t} else if ( type !== 'label' && type !== 'linear' ) {\n\t\t\t\tthrow new TypeError( format( 'invalid argument. Second argument must be a string value equal to either \"label\", \"probability\", or \"linear\". Value: `%s`.', type ) );\n\t\t\t}\n\t\t\tt = type;\n\t\t}\n\t\treturn model.predict( X, t );\n\t}\n}\n\n\n// EXPORTS //\n\nexport default incrBinaryClassification;\n"],"names":["LEARNING_RATE_METHODS","basic","constant","invscaling","pegasos","LOSS_METHODS","hinge","log","modifiedHuber","perceptron","squaredHinge","Model","N","opts","len","this","_N","_opts","_scaleFactor","_t","_learningRateMethod","learningRate","_lossMethod","loss","intercept","_weights","Float64Array","_coefficients","ndarray","setReadOnly","prototype","x","scale","s","w","gaxpy","shape","data","strides","offset","buf","stride","v","gdot","y","eta","_regularize","_dot","_add","params","pow","d","exp","lambda","_scale","max","factor","RangeError","format","dscal","setReadOnlyAccessor","c","dcopy","length","X","type","ndims","xbuf","ybuf","xsh","ysh","ord","ptr","sxn","sx","sy","ox","M","Y","i","order","push","numel","shape2strides","vind2bind","sigmoid","iset","validate","options","name","isObject","TypeError","hasOwnProp","isBoolean","isNonNegativeNumber","isArrayLikeObject","contains","LEARNING_RATES","join","isPositiveNumber","isNumber","LOSS_FUNCTIONS","incrBinaryClassification","model","err","isPositiveInteger","LEARNING_RATE_DEFAULTS","slice","arguments","accumulator","predict","coefficients","isVectorLike","nfeatures","update","sh","t","isndarrayLike","Error"],"mappings":";;g/EA4CA,IAEIA,EAAwB,CAC3BC,MAAS,qBACTC,SAAY,wBACZC,WAAc,8BACdC,QAAW,wBAERC,EAAe,CAClBC,MAAS,aACTC,IAAO,WACPC,cAAiB,qBACjBC,WAAc,kBACdC,aAAgB,qBAyBjB,SAASC,EAAOC,EAAGC,GAClB,IAAIC,EA0BJ,OAvBAC,KAAKC,GAAKJ,EACVG,KAAKE,MAAQJ,EAEbE,KAAKG,aAAe,EACpBH,KAAKI,GAAK,EAGVJ,KAAKK,oBAAsBpB,EAAuBa,EAAKQ,aAAc,IAGrEN,KAAKO,YAAcjB,EAAcQ,EAAKU,MAGtCT,EAAMF,EACDC,EAAKW,YACTV,GAAO,GAGRC,KAAKU,SAAW,IAAIC,EAAcZ,GAGlCC,KAAKY,cAAgB,IAAIC,EAAS,UAAW,IAAIF,EAAcZ,GAAO,CAAEA,GAAO,CAAE,GAAK,EAAG,aAElFC,KAcRc,EAAalB,EAAMmB,UAAW,QAAQ,SAAcC,EAAGC,GACtD,IAAIC,EAAID,EAAQjB,KAAKG,aACjBgB,EAAInB,KAAKU,SASb,OANAU,EAAOJ,EAAEK,MAAO,GAAKH,EAAGF,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,OAAQL,EAAG,EAAG,GAG3DnB,KAAKE,MAAMO,YACfU,EAAGnB,KAAKC,KAAQiB,GAEVlB,QAkBRc,EAAalB,EAAMmB,UAAW,sBAAsB,WACnD,OAAO,IAAS,GAAKf,KAAKI,OAY3BU,EAAalB,EAAMmB,UAAW,yBAAyB,WACtD,OAAOf,KAAKE,MAAMI,aAAc,MAejCQ,EAAalB,EAAMmB,UAAW,QAAQ,SAAcU,EAAKC,EAAQF,GAChE,IAAIG,EAAIC,EAAM5B,KAAKC,GAAID,KAAKU,SAAU,EAAG,EAAGe,EAAKC,EAAQF,GAKzD,OAJKxB,KAAKE,MAAMO,YACfkB,GAAK3B,KAAKU,SAAUV,KAAKC,KAE1B0B,GAAK3B,KAAKG,gBA+BXW,EAAalB,EAAMmB,UAAW,cAAc,SAAoBC,EAAGa,GAClE,IAAIC,EAUJ,OAPAA,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAGXD,EADH7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,QAC1B,GACdxB,KAAKiC,KAAMjB,EAAGa,EAAEC,GAEV9B,QAsBRc,EAAalB,EAAMmB,UAAW,+BAA+B,WAC5D,IAAImB,EAASlC,KAAKE,MAAMI,aACxB,OAAO4B,EAAQ,GAAMC,EAAKnC,KAAKI,GAAI8B,EAAQ,OA+B5CpB,EAAalB,EAAMmB,UAAW,YAAY,SAAkBC,EAAGa,GAC9D,IAAIrB,EACAsB,EACAM,EASJ,OAPAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAElBM,EAAIpC,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,QACzChB,EAAOqB,GAAM,EAAMQ,EAAKR,EAAEO,IAC1BpC,KAAKiC,KAAMjB,EAAGc,EAAItB,GAEXR,QAuCRc,EAAalB,EAAMmB,UAAW,sBAAsB,SAAwBC,EAAGa,GAC9E,IAAIC,EACAM,EAWJ,OATAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,IAElBM,EAAIP,EAAI7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,UACnC,EACTxB,KAAKiC,KAAMjB,EAAG,EAAIc,EAAID,GAEtB7B,KAAKiC,KAAMjB,EAAGc,GAAMD,EAAGO,EAAEP,IAEnB7B,QAkBRc,EAAalB,EAAMmB,UAAW,wBAAwB,WACrD,OAAO,GAAQf,KAAKE,MAAMoC,OAAOtC,KAAKI,OAsCvCU,EAAalB,EAAMmB,UAAW,mBAAmB,SAAqBC,EAAGa,GACxE,IAAIC,EAUJ,OAPAA,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,GAGXD,EADH7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,SACzB,GACfxB,KAAKiC,KAAMjB,EAAGa,EAAEC,GAEV9B,QAaRc,EAAalB,EAAMmB,UAAW,eAAe,SAAqBe,GACjE,IAAIQ,EAAStC,KAAKE,MAAMoC,OACxB,OAAKA,GAAU,GAGftC,KAAKuC,OAAQC,EAAK,EAAMV,EAAIQ,EArYJ,OAmYhBtC,QAiBTc,EAAalB,EAAMmB,UAAW,UAAU,SAAgB0B,GACvD,IAAIvB,EACJ,GAAKuB,GAAU,EACd,MAAM,IAAIC,WAAYC,EAAQ,sJAAuJF,IAUtL,OAPAvB,EAAIlB,KAAKG,cAzZM,QA4ZdyC,EAAO5C,KAAKC,GAAIiB,EAAGlB,KAAKU,SAAU,GAClCV,KAAKG,aAAe,GAErBH,KAAKG,cAAgBsC,EACdzC,QA8BRc,EAAalB,EAAMmB,UAAW,qBAAqB,SAA2BC,EAAGa,GAChF,IAAIC,EACAM,EASJ,OAPAN,EAAM9B,KAAMA,KAAKK,uBACjBL,KAAK+B,YAAaD,IAElBM,EAAIP,EAAI7B,KAAKgC,KAAMhB,EAAEM,KAAMN,EAAEO,QAAS,GAAKP,EAAEQ,SACpC,GACRxB,KAAKiC,KAAMjB,EAAGc,GAAMD,EAAGO,EAAEP,IAEnB7B,QAYR6C,EAAqBjD,EAAMmB,UAAW,gBAAgB,WACrD,IAAI+B,EAAI9C,KAAKY,cAAcU,KACvBH,EAAInB,KAAKU,SAGb,OAFAqC,EAAO5B,EAAE6B,OAAQ7B,EAAG,EAAG2B,EAAG,GAC1BF,EAAO5C,KAAKC,GAAID,KAAKG,aAAc2C,EAAG,GAC/B9C,KAAKY,iBAWbiC,EAAqBjD,EAAMmB,UAAW,aAAa,WAClD,OAAOf,KAAKC,MAcba,EAAalB,EAAMmB,UAAW,WAAW,SAAkBkC,EAAGC,GAC7D,IAAIC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAC,EACAjE,EACAkE,EACApC,EACAqC,EAaJ,IAVAZ,EAAOH,EAAE3B,KACTgC,EAAML,EAAE5B,MACRsC,EAAKV,EAAE1B,QACPsC,EAAKZ,EAAEzB,OACPgC,EAAMP,EAAEgB,MAERd,EAAQG,EAAIN,OAAS,EAGrBO,EAAM,GACAS,EAAI,EAAGA,EAAIb,EAAOa,IACvBT,EAAIW,KAAMZ,EAAKU,IAiBhB,IAde,IAAVb,GACJW,EAAI,EACJT,EAAO,IAAI1C,EAAc,GACzBiD,EAAK,CAAE,KAEPE,EAAIK,EAAOZ,GACXF,EAAO,IAAI1C,EAAcmD,GACzBF,EAAKQ,EAAeb,EAAKC,IAE1BO,EAAI,IAAIlD,EAAS,OAAQwC,EAAME,EAAKK,EAAI,EAAGJ,GAG3C3D,EAAIG,KAAKC,GACTyD,EAAMC,EAAIR,GACJa,EAAI,EAAGA,EAAIF,EAAGE,IAEnBP,EAAMY,EAAWf,EAAKK,EAAIE,EAAIL,EAAKQ,EAAEnE,EAAG,SAGxC8B,EAAI3B,KAAKgC,KAAMoB,EAAMM,EAAKD,GAGZ,UAATP,EACJvB,EAAMA,EAAI,EAAM,GAAK,EACD,gBAATuB,IACXvB,EAAI2C,EAAS3C,IAIC,IAAVwB,EACJY,EAAEQ,KAAM5C,GAERoC,EAAEQ,KAAMP,EAAGrC,GAGb,OAAOoC,KAcRjD,EAAalB,EAAMmB,UAAW,UAAU,SAAiBC,EAAGa,GAE3D,OADA7B,KAAKI,IAAM,EACJJ,KAAMA,KAAKO,aAAeS,EAAGa,yNC5jBrC,SAAS2C,EAAU1E,EAAM2E,GACxB,IAAIC,EACJ,IAAMC,EAAUF,GACf,OAAO,IAAIG,UAAWjC,EAAQ,QAAS8B,IAExC,GAAKI,EAAYJ,EAAS,eACzB3E,EAAKW,UAAYgE,EAAQhE,WACnBqE,EAAWhF,EAAKW,YACrB,OAAO,IAAImE,UAAWjC,EAAQ,QAAS,YAAa7C,EAAKW,YAG3D,GAAKoE,EAAYJ,EAAS,YACzB3E,EAAKwC,OAASmC,EAAQnC,QAChByC,EAAqBjF,EAAKwC,SAC/B,OAAO,IAAIsC,UAAWjC,EAAQ,QAAS,SAAU7C,EAAKwC,SAGxD,GAAKuC,EAAYJ,EAAS,gBAAmB,CAC5C,IAAMO,EAAmBP,EAAQnE,cAChC,OAAO,IAAIsE,UAAWjC,EAAQ,QAAS,eAAgB8B,EAAQnE,eAIhE,GAFAoE,EAAOD,EAAQnE,aAAc,GAC7BR,EAAKQ,aAAc,GAAMoE,GACnBO,EAAUC,EAAgBR,GAC/B,OAAO,IAAIE,UAAWjC,EAAQ,QAAS,eAAgBuC,EAAeC,KAAM,QAAUT,IAEvF,GAAKD,EAAQnE,aAAa0C,OAAS,IACpB,aAAT0B,GAAgC,eAATA,KAC3B5E,EAAKQ,aAAc,GAAMmE,EAAQnE,aAAc,IACzC8E,EAAkBtF,EAAKQ,aAAc,KAC1C,OAAO,IAAIsE,UAAWjC,EAAQ,8EAA+E,eAAgB7C,EAAKQ,aAAc,KAInJ,GAAKmE,EAAQnE,aAAa0C,OAAS,GACpB,eAAT0B,IACJ5E,EAAKQ,aAAc,GAAMmE,EAAQnE,aAAc,IACzC+E,EAAUvF,EAAKQ,aAAc,KAClC,OAAO,IAAIsE,UAAWjC,EAAQ,oEAAqE,eAAgB7C,EAAKQ,aAAc,KAK1I,OAAKuE,EAAYJ,EAAS,UACzB3E,EAAKU,KAAOiE,EAAQjE,MACdyE,EAAUK,EAAgBxF,EAAKU,OAC7B,IAAIoE,UAAWjC,EAAQ,QAAS,OAAQ2C,EAAeH,KAAM,QAAUrF,EAAKU,OAG9E,KCxBR,SAAS+E,EAA0B1F,EAAG4E,GACrC,IAAIe,EACA1F,EACA2F,EAEJ,IAAMC,EAAmB7F,GACxB,MAAM,IAAI+E,UAAWjC,EAAQ,QAAS9C,IAQvC,GANAC,EAAO,CACNW,WAAa,EACb6B,OAAU,KACVhC,aAAgBqF,EAAiC,MAACC,QAClDpF,KAAQ,OAEJqF,UAAU7C,OAAS,IACvByC,EAAMjB,EAAU1E,EAAM2E,IAErB,MAAMgB,EAQR,OALAD,EAAQ,IAAI5F,EAAOC,EAAGC,GAGtBgB,EAAagF,EAAa,UAAWC,GAE9BD,EA2BP,SAASA,EAAa9E,EAAGa,GACxB,GAA0B,IAArBgE,UAAU7C,OACd,OAAOwC,EAAMQ,aAEd,IAAMC,EAAcjF,GACnB,MAAM,IAAI4D,UAAWjC,EAAQ,QAAS3B,IAEvC,IAAY,IAAPa,GAAkB,IAANA,EAChB,MAAM,IAAI+C,UAAWjC,EAAQ,QAASd,IAEvC,GAAKb,EAAEK,MAAO,KAAQmE,EAAMU,UAC3B,MAAM,IAAItB,UAAWjC,EAAQ,wGAAyG6C,EAAMU,UAAWlF,EAAEK,MAAO,KAGjK,OADAmE,EAAMW,OAAQnF,EAAGa,GACV2D,EAAMQ,aA+Bd,SAASD,EAAS9C,EAAGC,GACpB,IAAIkD,EACAC,EACJ,IAAMC,EAAerD,GACpB,MAAM,IAAI2B,UAAWjC,EAAQ,QAASM,IAGvC,IADAmD,EAAKnD,EAAE5B,OACE+E,EAAGpD,OAAO,KAAQnD,EAC1B,MAAM,IAAI+E,UAAWjC,EAAQ,6GAA8G9C,EAAGuG,EAAIA,EAAGpD,OAAO,KAG7J,GADAqD,EAAI,QACCR,UAAU7C,OAAS,EAAI,CAC3B,GAAc,gBAATE,GACJ,GAAmB,QAAdpD,EAAKU,MAAgC,kBAAdV,EAAKU,KAChC,MAAM,IAAI+F,MAAO5D,EAAQ,0MAA2M,CAAE,MAAO,iBAAkBwC,KAAM,QAAUrF,EAAKU,YAE/Q,GAAc,UAAT0C,GAA6B,WAATA,EAC/B,MAAM,IAAI0B,UAAWjC,EAAQ,6HAA8HO,IAE5JmD,EAAInD,EAEL,OAAOsC,EAAMO,QAAS9C,EAAGoD"}